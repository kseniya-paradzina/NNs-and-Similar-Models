{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2psHklGRjNS"
   },
   "source": [
    "# EcoSort Waste Management Assistant\n",
    "# Module 8 Summative Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L1jGlvHRjNV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "You are a data scientist at \"EcoSort,\" a technology company that specializes in developing AI solutions for waste management. EcoSort has partnered with Metro City's waste management department to develop an intelligent waste management assistant that can help residents properly dispose of waste items so less time is spent sorting material at facilities.\n",
    "\n",
    "This assistant needs to:\n",
    "\n",
    "1. Identify waste materials from images uploaded by residents (CNN)\n",
    "2. Classify waste items based on text descriptions provided by residents (RNN/Transformer)\n",
    "3. Generate specific recycling instructions based on identified waste type and city policies (Generative Transformer with RAG)\n",
    "\n",
    "Your task is to build this integrated system using the RealWaste dataset along with generated text data that simulates real-world waste management operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHzCo330RjNW"
   },
   "source": [
    "## Part 1: Dataset Exploration and Preparation\n",
    "\n",
    "In this section, you will explore and prepare the datasets for your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-3mzMwFRjNW"
   },
   "source": [
    "### 1.1 Load and Explore the RealWaste Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 4752\n",
      "Num classes : 9\n",
      "Classes     : Cardboard, Food Organics, Glass, Metal, Miscellaneous Trash, Paper, Plastic, Textile Trash, Vegetation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cardboard</th>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food Organics</th>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glass</th>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metal</th>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miscellaneous Trash</th>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plastic</th>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Textile Trash</th>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vegetation</th>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "label                     \n",
       "Cardboard              461\n",
       "Food Organics          411\n",
       "Glass                  420\n",
       "Metal                  790\n",
       "Miscellaneous Trash    495\n",
       "Paper                  500\n",
       "Plastic                921\n",
       "Textile Trash          318\n",
       "Vegetation             436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== 1.1 Load and Explore the RealWaste Dataset ====\n",
    "\n",
    "# --- Standard library ---\n",
    "import os, sys, io, re, json, time, random, itertools, math, glob, traceback\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# --- Scientific stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# --- Persistence / utilities ---\n",
    "import joblib\n",
    "\n",
    "# --- Scikit-learn (classic ML + metrics + features) ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(42)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---- CONFIG: set your images root here\n",
    "IMAGES_ROOT = Path(\"/Users/Kseniya/Downloads/realwaste-main/RealWaste\")  # change if needed\n",
    "\n",
    "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\")\n",
    "\n",
    "def list_images_by_class(root: Path):\n",
    "    rows = []\n",
    "    if not root.exists():\n",
    "        return pd.DataFrame(columns=[\"path\",\"label\",\"width\",\"height\",\"aspect\",\"area\",\"sharpness\",\"colorfulness\"])\n",
    "    for cls_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
    "        label = cls_dir.name\n",
    "        for p in cls_dir.rglob(\"*\"):\n",
    "            if p.suffix.lower() in IMG_EXTS:\n",
    "                rows.append({\"path\": str(p), \"label\": label})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def pil_read_size(p: str):\n",
    "    try:\n",
    "        with Image.open(p) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            w, h = im.size\n",
    "            return w, h\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def variance_of_laplacian_np(img_rgb: np.ndarray) -> float:\n",
    "    \"\"\"Sharpness proxy without OpenCV. img_rgb: HxWx3 (0..255).\"\"\"\n",
    "    # convert to gray\n",
    "    g = (0.299*img_rgb[...,0] + 0.587*img_rgb[...,1] + 0.114*img_rgb[...,2]).astype(np.float32)\n",
    "    # 3x3 Laplacian kernel\n",
    "    k = np.array([[0,1,0],[1,-4,1],[0,1,0]], dtype=np.float32)\n",
    "    # pad and convolve\n",
    "    pad = 1\n",
    "    gp = np.pad(g, pad, mode=\"edge\")\n",
    "    H, W = g.shape\n",
    "    out = np.zeros_like(g)\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            region = gp[i:i+3, j:j+3]\n",
    "            out[i,j] = np.sum(region * k)\n",
    "    return float(out.var())\n",
    "\n",
    "def colorfulness_hasler(img_rgb: np.ndarray) -> float:\n",
    "    \"\"\"Hasler & Suesstrunk colorfulness metric.\"\"\"\n",
    "    R, G, B = img_rgb[...,0].astype(np.float32), img_rgb[...,1].astype(np.float32), img_rgb[...,2].astype(np.float32)\n",
    "    rg = np.abs(R - G)\n",
    "    yb = np.abs(0.5*(R + G) - B)\n",
    "    std_rg, std_yb = rg.std(), yb.std()\n",
    "    mean_rg, mean_yb = rg.mean(), yb.mean()\n",
    "    return math.sqrt(std_rg**2 + std_yb**2) + 0.3*math.sqrt(mean_rg**2 + mean_yb**2)\n",
    "\n",
    "def compute_image_stats(df: pd.DataFrame, sample_for_quality=300):\n",
    "    \"\"\"Adds width/height/aspect/area + sharpness & colorfulness (on a sample to save time).\"\"\"\n",
    "    ws, hs = [], []\n",
    "    for p in df[\"path\"]:\n",
    "        w, h = pil_read_size(p)\n",
    "        ws.append(w); hs.append(h)\n",
    "    df[\"width\"], df[\"height\"] = ws, hs\n",
    "    df = df.dropna(subset=[\"width\",\"height\"])\n",
    "    df[\"width\"] = df[\"width\"].astype(int)\n",
    "    df[\"height\"] = df[\"height\"].astype(int)\n",
    "    df[\"aspect\"] = (df[\"width\"] / df[\"height\"]).round(4)\n",
    "    df[\"area\"] = df[\"width\"] * df[\"height\"]\n",
    "\n",
    "    # quality metrics on a subset\n",
    "    idxs = df.sample(min(sample_for_quality, len(df)), random_state=42).index if len(df) else []\n",
    "    sharpness = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "    colorfulness = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "    for i in idxs:\n",
    "        try:\n",
    "            with Image.open(df.loc[i,\"path\"]) as im:\n",
    "                im = im.convert(\"RGB\")\n",
    "                arr = np.array(im)\n",
    "            sharpness[i] = variance_of_laplacian_np(arr)\n",
    "            colorfulness[i] = colorfulness_hasler(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "    df[\"sharpness\"] = sharpness\n",
    "    df[\"colorfulness\"] = colorfulness\n",
    "    return df\n",
    "\n",
    "def show_examples(df: pd.DataFrame, per_class=3, img_size=160):\n",
    "    classes = sorted(df[\"label\"].unique())\n",
    "    for cls in classes:\n",
    "        sub = df[df[\"label\"] == cls].sample(min(per_class, len(df[df[\"label\"]==cls])), random_state=42)\n",
    "        if len(sub)==0: \n",
    "            continue\n",
    "        plt.figure(figsize=(per_class*2.3, 2.3))\n",
    "        for i, (_, row) in enumerate(sub.iterrows(), start=1):\n",
    "            plt.subplot(1, len(sub), i)\n",
    "            try:\n",
    "                im = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "                im = im.resize((img_size, img_size))\n",
    "                plt.imshow(im)\n",
    "                plt.title(cls, fontsize=9)\n",
    "                plt.axis(\"off\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---- Run exploration\n",
    "df = list_images_by_class(IMAGES_ROOT)\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(f\"[!] No images found under: {IMAGES_ROOT.resolve()}\")\n",
    "    print(\"Expected layout: data/images/<class_name>/*.jpg ... with classes:\")\n",
    "    print(\"paper, cardboard, biological, metal, plastic, green-glass, brown-glass, white-glass\")\n",
    "else:\n",
    "    # Basic structure\n",
    "    print(\"Total images:\", len(df))\n",
    "    print(\"Num classes :\", df['label'].nunique())\n",
    "    print(\"Classes     :\", \", \".join(sorted(df['label'].unique().tolist())))\n",
    "\n",
    "    # Category distribution\n",
    "    counts = df[\"label\"].value_counts().sort_index()\n",
    "    display(pd.DataFrame({\"count\": counts}))\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Distribution of waste categories\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Image characteristics\n",
    "    df = compute_image_stats(df)\n",
    "\n",
    "    # Resolution histogram (area)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    (df[\"area\"]).plot(kind=\"hist\", bins=30)\n",
    "    plt.title(\"Image area (pixels) distribution\")\n",
    "    plt.xlabel(\"width * height\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Aspect ratio histogram\n",
    "    plt.figure(figsize=(8,4))\n",
    "    df[\"aspect\"].plot(kind=\"hist\", bins=30)\n",
    "    plt.title(\"Aspect ratio distribution (width/height)\")\n",
    "    plt.xlabel(\"Aspect ratio\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Sharpness (variance of Laplacian) on sample\n",
    "    if df[\"sharpness\"].notna().sum() > 0:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        df[\"sharpness\"].dropna().plot(kind=\"hist\", bins=30)\n",
    "        plt.title(\"Sharpness distribution (variance of Laplacian, sample)\")\n",
    "        plt.xlabel(\"Sharpness\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Sharpness (sample) — mean:\", round(df[\"sharpness\"].mean(), 3),\n",
    "              \"median:\", round(df[\"sharpness\"].median(), 3))\n",
    "\n",
    "    # Colorfulness on sample\n",
    "    if df[\"colorfulness\"].notna().sum() > 0:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        df[\"colorfulness\"].dropna().plot(kind=\"hist\", bins=30)\n",
    "        plt.title(\"Colorfulness distribution (sample)\")\n",
    "        plt.xlabel(\"Colorfulness\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Colorfulness (sample) — mean:\", round(df[\"colorfulness\"].mean(), 3),\n",
    "              \"median:\", round(df[\"colorfulness\"].median(), 3))\n",
    "\n",
    "    # Few examples per class\n",
    "    show_examples(df, per_class=3)\n",
    "\n",
    "    # Quick summary table per class\n",
    "    agg = df.groupby(\"label\").agg(\n",
    "        n=(\"path\",\"count\"),\n",
    "        w_mean=(\"width\",\"mean\"),\n",
    "        h_mean=(\"height\",\"mean\"),\n",
    "        area_median=(\"area\",\"median\"),\n",
    "        aspect_mean=(\"aspect\",\"mean\"),\n",
    "        sharp_mean=(\"sharpness\",\"mean\"),\n",
    "        color_mean=(\"colorfulness\",\"mean\"),\n",
    "    ).round(2)\n",
    "    display(agg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR-vCTxIRjNY"
   },
   "source": [
    "### 1.2 Explore Text Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1.2a — Explore waste_descriptions.csv ====\n",
    "# This cell:\n",
    "# - Locates and loads the CSV\n",
    "# - Detects key columns (text/label/etc.)\n",
    "# - Shows category distribution and description lengths\n",
    "# - Lists top unigrams/bigrams and safe per-class TF-IDF keywords\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Robust location (edit or add your own paths)\n",
    "candidates = [\n",
    "    Path(\"waste_descriptions.csv\"),\n",
    "    Path(\"./data/waste_descriptions.csv\"),\n",
    "    Path(\"../data/waste_descriptions.csv\"),\n",
    "]\n",
    "desc_path = next((p for p in candidates if p.exists()), None)\n",
    "if desc_path is None:\n",
    "    raise FileNotFoundError(\"waste_descriptions.csv not found. Place it next to the notebook or in ./data.\")\n",
    "\n",
    "df = pd.read_csv(desc_path)\n",
    "print(\"Rows:\", len(df), \"| Columns:\", list(df.columns))\n",
    "\n",
    "# Column picker: exact match first, then substring\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    # exact\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want:\n",
    "                return orig\n",
    "    # substring\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col = pick(df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "label_col = pick(df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "disp_col  = pick(df.columns, [\"disposal_instruction\",\"proper_disposal\",\"disposal\",\"instructions\"])\n",
    "conf_col  = pick(df.columns, [\"common_confusion\",\"confusion\"])\n",
    "comp_col  = pick(df.columns, [\"material_composition\",\"composition\"])\n",
    "\n",
    "print(\"Detected columns:\", {\"text\":text_col, \"label\":label_col, \"disposal\":disp_col, \"confusion\":conf_col, \"composition\":comp_col})\n",
    "assert text_col and label_col, \"Need at least a text column and a label column.\"\n",
    "\n",
    "# Basic cleaning\n",
    "df[text_col]  = df[text_col].astype(str).str.strip()\n",
    "df[label_col] = df[label_col].astype(str).str.strip()\n",
    "df = df[df[text_col].str.len() > 0].drop_duplicates(subset=[text_col, label_col]).reset_index(drop=True)\n",
    "\n",
    "# Category distribution\n",
    "counts = df[label_col].value_counts().sort_index()\n",
    "display(pd.DataFrame({\"count\": counts}))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "counts.plot(kind=\"bar\")\n",
    "plt.title(\"Distribution of waste categories\")\n",
    "plt.xlabel(\"Category\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Description length analysis\n",
    "desc_len = df[text_col].str.split().apply(len)\n",
    "print(\"Description length — mean:\", round(desc_len.mean(),2), \n",
    "      \"median:\", int(desc_len.median()), \n",
    "      \"max:\", int(desc_len.max()))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(desc_len, bins=30)\n",
    "plt.title(\"Description length (words)\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Disposal instruction length (if present)\n",
    "if disp_col:\n",
    "    disp_len = df[disp_col].dropna().astype(str).str.split().apply(len)\n",
    "    print(\"Disposal instruction length — mean:\", round(disp_len.mean(),2), \n",
    "          \"median:\", int(disp_len.median()))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(disp_len, bins=30)\n",
    "    plt.title(\"Disposal instruction length (words)\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Top unigrams/bigrams (global) with safe fallback\n",
    "def top_terms(corpus, n=20, ngram=(1,1), stop=\"english\"):\n",
    "    try:\n",
    "        vec = CountVectorizer(lowercase=True, stop_words=stop, ngram_range=ngram, min_df=2)\n",
    "        X = vec.fit_transform(corpus)\n",
    "    except ValueError:\n",
    "        vec = CountVectorizer(lowercase=True, stop_words=stop, ngram_range=ngram, min_df=1)\n",
    "        X = vec.fit_transform(corpus)\n",
    "    sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    top_ix = np.argsort(-sums)[:min(n, sums.size)]\n",
    "    return pd.DataFrame({\"term\": vocab[top_ix], \"count\": sums[top_ix]})\n",
    "\n",
    "print(\"\\nTop unigrams:\")\n",
    "display(top_terms(df[text_col], n=20, ngram=(1,1)))\n",
    "\n",
    "print(\"Top bigrams:\")\n",
    "display(top_terms(df[text_col], n=20, ngram=(2,2)))\n",
    "\n",
    "# SAFE per-class TF-IDF (handles small groups)\n",
    "def top_tfidf_per_class_safe(frame, text_col, label_col, topk=10):\n",
    "    rows = []\n",
    "    for label, sub in frame.groupby(label_col):\n",
    "        docs = sub[text_col].fillna(\"\").astype(str).tolist()\n",
    "        n = len(docs)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        min_df = 2 if n >= 5 else 1\n",
    "        max_df = 0.9 if n >= 5 else 1.0\n",
    "\n",
    "        vec = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                              ngram_range=(1,2), min_df=min_df, max_df=max_df)\n",
    "        try:\n",
    "            X = vec.fit_transform(docs)\n",
    "        except ValueError:\n",
    "            vec = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                                  ngram_range=(1,1), min_df=1, max_df=1.0)\n",
    "            X = vec.fit_transform(docs)\n",
    "\n",
    "        means = np.asarray(X.mean(axis=0)).ravel()\n",
    "        if means.size == 0:\n",
    "            continue\n",
    "\n",
    "        top_ix = np.argsort(-means)[:min(topk, means.size)]\n",
    "        vocab = np.array(vec.get_feature_names_out())\n",
    "        for t, s in zip(vocab[top_ix], means[top_ix]):\n",
    "            rows.append({\"category\": label, \"term\": t, \"mean_tfidf\": float(s)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nTop TF-IDF terms per category (safe):\")\n",
    "display(top_tfidf_per_class_safe(df, text_col, label_col, topk=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1.2b — Explore waste_policy_documents (JSON only) ====\n",
    "# This cell:\n",
    "# - Loads policy docs from a JSON file\n",
    "# - Reports document lengths, category coverage, jurisdictions/types\n",
    "# - Shows top TF-IDF terms (global) and safe per-category keywords\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Locate & load JSON (edit path if needed)\n",
    "candidates = [\n",
    "    Path(\"waste_policy_documents.json\"),\n",
    "    Path(\"./data/waste_policy_documents.json\"),\n",
    "    Path(\"../data/waste_policy_documents.json\"),\n",
    "]\n",
    "pol_json = next((p for p in candidates if p.exists()), None)\n",
    "if pol_json is None:\n",
    "    raise FileNotFoundError(\"Policy JSON not found. Place it next to the notebook or in ./data.\")\n",
    "\n",
    "with open(pol_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "pdf = pd.json_normalize(data)\n",
    "\n",
    "print(\"Rows:\", len(pdf), \"| Columns:\", list(pdf.columns))\n",
    "\n",
    "# Column detection (exact first, then substring)\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want:\n",
    "                return orig\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col = pick(pdf.columns, [\"document_text\",\"text\",\"body\",\"content\"])\n",
    "cats_col = pick(pdf.columns, [\"categories_covered\",\"category\",\"categories\"])\n",
    "jur_col  = pick(pdf.columns, [\"jurisdiction\",\"city\",\"region\"])\n",
    "type_col = pick(pdf.columns, [\"policy_type\",\"type\"])\n",
    "date_col = pick(pdf.columns, [\"effective_date\",\"date\",\"updated_at\"])\n",
    "\n",
    "print(\"Detected columns:\", {\"text\":text_col, \"categories\":cats_col, \"jurisdiction\":jur_col, \"policy_type\":type_col, \"date\":date_col})\n",
    "assert text_col, \"Need a text column for the policy content.\"\n",
    "\n",
    "# Clean and de-duplicate\n",
    "pdf[text_col] = pdf[text_col].fillna(\"\").astype(str).str.strip()\n",
    "pdf = pdf[pdf[text_col].str.len() > 0].drop_duplicates(subset=[text_col]).reset_index(drop=True)\n",
    "\n",
    "# Document length distribution\n",
    "pdf[\"_words\"] = pdf[text_col].str.split().apply(len)\n",
    "print(\"Document length (words) — mean:\", round(pdf[\"_words\"].mean(),2), \n",
    "      \"median:\", int(pdf[\"_words\"].median()))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(pdf[\"_words\"], bins=30)\n",
    "plt.title(\"Policy document length (words)\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Helper to coerce category field into a list\n",
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, str) and x.strip().startswith(\"[\"):\n",
    "        try:\n",
    "            arr = json.loads(x)\n",
    "            if isinstance(arr, list):\n",
    "                return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [x]\n",
    "\n",
    "# Category coverage (if available)\n",
    "if cats_col:\n",
    "    pdf[\"_categories_list\"] = pdf[cats_col].apply(to_list)\n",
    "    cat_rows = [{\"category\": c} for lst in pdf[\"_categories_list\"] for c in lst]\n",
    "    cdf = pd.DataFrame(cat_rows)\n",
    "    if len(cdf):\n",
    "        counts = cdf[\"category\"].astype(str).value_counts().sort_index()\n",
    "        display(pd.DataFrame({\"count\": counts}))\n",
    "        plt.figure(figsize=(8,4))\n",
    "        counts.plot(kind=\"bar\")\n",
    "        plt.title(\"Categories covered in policy documents\")\n",
    "        plt.xlabel(\"Category\"); plt.ylabel(\"Count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    pdf[\"_categories_list\"] = [[] for _ in range(len(pdf))]\n",
    "\n",
    "# Jurisdictions / policy types\n",
    "if jur_col:\n",
    "    pdf[jur_col] = pdf[jur_col].fillna(\"Unknown\").astype(str).str.strip()\n",
    "    display(pd.DataFrame({\"jurisdiction_count\": pdf[jur_col].value_counts()}))\n",
    "if type_col:\n",
    "    pdf[type_col] = pdf[type_col].fillna(\"Unknown\").astype(str).str.strip()\n",
    "    display(pd.DataFrame({\"policy_type_count\": pdf[type_col].value_counts()}))\n",
    "\n",
    "# Document recency by month (optional)\n",
    "if date_col:\n",
    "    pdf[\"_date\"] = pd.to_datetime(pdf[date_col], errors=\"coerce\")\n",
    "    if pdf[\"_date\"].notna().any():\n",
    "        plt.figure(figsize=(8,3))\n",
    "        pdf[\"_date\"].dt.to_period(\"M\").value_counts().sort_index().plot(kind=\"bar\")\n",
    "        plt.title(\"Policy docs by month\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "# Global language profile: top TF-IDF terms (robust)\n",
    "try:\n",
    "    vec_global = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                                 ngram_range=(1,2), min_df=2, max_df=0.9,\n",
    "                                 max_features=50000)\n",
    "    Xg = vec_global.fit_transform(pdf[text_col])\n",
    "except ValueError:\n",
    "    vec_global = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                                 ngram_range=(1,1), min_df=1, max_df=1.0,\n",
    "                                 max_features=20000)\n",
    "    Xg = vec_global.fit_transform(pdf[text_col])\n",
    "\n",
    "means_g = np.asarray(Xg.mean(axis=0)).ravel()\n",
    "vocab_g = np.array(vec_global.get_feature_names_out())\n",
    "top_ix_g = np.argsort(-means_g)[:min(30, means_g.size)]\n",
    "display(pd.DataFrame({\"term\": vocab_g[top_ix_g], \"mean_tfidf\": means_g[top_ix_g]}))\n",
    "\n",
    "# SAFE per-category keywords (handles small groups)\n",
    "if cats_col:\n",
    "    safe_rows = []\n",
    "    cats = sorted({c for lst in pdf[\"_categories_list\"] for c in lst})\n",
    "    for c in cats:\n",
    "        mask = pdf[\"_categories_list\"].apply(lambda xs: c in xs)\n",
    "        docs = pdf.loc[mask, text_col].tolist()\n",
    "        n = len(docs)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        min_df = 2 if n >= 5 else 1\n",
    "        max_df = 0.9 if n >= 5 else 1.0\n",
    "\n",
    "        try:\n",
    "            vec = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                                  ngram_range=(1,2), min_df=min_df, max_df=max_df,\n",
    "                                  max_features=20000)\n",
    "            Xc = vec.fit_transform(docs)\n",
    "        except ValueError:\n",
    "            vec = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                                  ngram_range=(1,1), min_df=1, max_df=1.0,\n",
    "                                  max_features=10000)\n",
    "            Xc = vec.fit_transform(docs)\n",
    "\n",
    "        means_c = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "        if means_c.size == 0:\n",
    "            continue\n",
    "\n",
    "        top_ix = np.argsort(-means_c)[:min(10, means_c.size)]\n",
    "        vocab_c = np.array(vec.get_feature_names_out())\n",
    "        for t, s in zip(vocab_c[top_ix], means_c[top_ix]):\n",
    "            safe_rows.append({\"category\": str(c), \"term\": t, \"mean_tfidf\": float(s)})\n",
    "\n",
    "    if safe_rows:\n",
    "        display(pd.DataFrame(safe_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj9uiWLuRjNZ"
   },
   "source": [
    "### 1.3 Create Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1.3 — Create Data Pipelines (TensorFlow, stratified train/val/test) ====\n",
    "# What this cell does:\n",
    "# - Scans a directory of class subfolders (e.g., RealWaste/paper, .../plastic, etc.)\n",
    "# - Builds stratified splits: 70% train, 15% val, 15% test\n",
    "# - Creates performant tf.data pipelines with caching/prefetch\n",
    "# - Adds optional light data augmentation for training\n",
    "\n",
    "import os, random, math, pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "data_dir = pathlib.Path(\"/Users/Kseniya/Downloads/realwaste-main/RealWaste\")  # <-- set this to the folder that has the 8 class subfolders\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH  = 224\n",
    "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.70, 0.15, 0.15  # must sum to 1.0\n",
    "ALLOWED_EXTS = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\")\n",
    "# ----------------------------\n",
    "\n",
    "# Validate directory\n",
    "assert data_dir.exists() and data_dir.is_dir(), f\"Directory not found: {data_dir}\"\n",
    "\n",
    "# List classes (subfolders)\n",
    "class_names = sorted([p.name for p in data_dir.iterdir() if p.is_dir()])\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "assert num_classes > 0, \"No class subfolders found.\"\n",
    "\n",
    "# Gather filepaths + labels\n",
    "filepaths, labels_str = [], []\n",
    "for cls in class_names:\n",
    "    cls_dir = data_dir / cls\n",
    "    for root, _, files in os.walk(cls_dir):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(ALLOWED_EXTS):\n",
    "                filepaths.append(str(pathlib.Path(root) / fname))\n",
    "                labels_str.append(cls)\n",
    "\n",
    "image_count = len(filepaths)\n",
    "print(f\"Total images found: {image_count}\")\n",
    "assert image_count > 0, f\"No images with {ALLOWED_EXTS} found under {data_dir}\"\n",
    "\n",
    "# Map class -> index\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "labels_idx = np.array([class_to_idx[s] for s in labels_str], dtype=np.int32)\n",
    "\n",
    "# Stratified split per class\n",
    "def stratified_split(paths, y, train_frac, val_frac, test_frac, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    paths, y = np.array(paths), np.array(y)\n",
    "    train_paths, val_paths, test_paths = [], [], []\n",
    "    train_labels, val_labels, test_labels = [], [], []\n",
    "\n",
    "    for cls_idx in sorted(set(y.tolist())):\n",
    "        cls_mask = (y == cls_idx)\n",
    "        cls_paths = paths[cls_mask]\n",
    "        cls_labels = y[cls_mask]\n",
    "        n = len(cls_paths)\n",
    "        idxs = np.arange(n)\n",
    "        rng.shuffle(idxs)\n",
    "\n",
    "        n_train = int(n * train_frac)\n",
    "        n_val   = int(n * val_frac)\n",
    "        n_test  = n - n_train - n_val  # remainder to test\n",
    "\n",
    "        tr_idx = idxs[:n_train]\n",
    "        va_idx = idxs[n_train:n_train+n_val]\n",
    "        te_idx = idxs[n_train+n_val:]\n",
    "\n",
    "        for i in tr_idx:\n",
    "            train_paths.append(cls_paths[i]); train_labels.append(int(cls_labels[i]))\n",
    "        for i in va_idx:\n",
    "            val_paths.append(cls_paths[i]);   val_labels.append(int(cls_labels[i]))\n",
    "        for i in te_idx:\n",
    "            test_paths.append(cls_paths[i]);  test_labels.append(int(cls_labels[i]))\n",
    "\n",
    "    return (\n",
    "        (train_paths, np.array(train_labels)),\n",
    "        (val_paths,   np.array(val_labels)),\n",
    "        (test_paths,  np.array(test_labels)),\n",
    "    )\n",
    "\n",
    "(train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \\\n",
    "    stratified_split(filepaths, labels_idx, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, seed=42)\n",
    "\n",
    "print(f\"Split sizes -> train: {len(train_paths)}, val: {len(val_paths)}, test: {len(test_paths)}\")\n",
    "\n",
    "# Optional: sanity check class balance\n",
    "def counts_per_split(lbls, split_name):\n",
    "    uniq, cnts = np.unique(lbls, return_counts=True)\n",
    "    d = {class_names[int(k)]: int(v) for k, v in zip(uniq, cnts)}\n",
    "    print(f\"{split_name} per-class counts: {d}\")\n",
    "counts_per_split(train_labels, \"train\")\n",
    "counts_per_split(val_labels,   \"val\")\n",
    "counts_per_split(test_labels,  \"test\")\n",
    "\n",
    "# TF helpers\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_and_preprocess(path, label):\n",
    "    # Read and decode image\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # scale to [0,1]; adjust if you do model-specific preprocessing\n",
    "    y = tf.one_hot(label, depth=num_classes)\n",
    "    return img, y\n",
    "\n",
    "# Data augmentation for training (light and safe)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.03),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "def make_dataset(paths, labels, training: bool):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if training:\n",
    "        # Shuffle BEFORE mapping for better randomness over filenames\n",
    "        shuffle_buf = max(1000, len(paths))\n",
    "        ds = ds.shuffle(shuffle_buf, seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds      = make_dataset(train_paths, train_labels, training=True)\n",
    "validation_ds = make_dataset(val_paths,   val_labels,   training=False)\n",
    "test_ds       = make_dataset(test_paths,  test_labels,  training=False)\n",
    "\n",
    "# Cardinalities\n",
    "print(\"Batches ->\",\n",
    "      \"train:\",       tf.data.experimental.cardinality(train_ds).numpy(),\n",
    "      \"| validation:\", tf.data.experimental.cardinality(validation_ds).numpy(),\n",
    "      \"| test:\",       tf.data.experimental.cardinality(test_ds).numpy())\n",
    "\n",
    "# Class weights (optional, useful if classes are imbalanced)\n",
    "from collections import Counter\n",
    "train_counts = Counter(train_labels.tolist())\n",
    "total = sum(train_counts.values())\n",
    "class_weights = {i: total/(num_classes * train_counts.get(i, 1)) for i in range(num_classes)}\n",
    "print(\"Suggested class_weights:\", {class_names[i]: round(w, 3) for i, w in class_weights.items()})\n",
    "\n",
    "# Expose for later parts\n",
    "PIPELINE_INFO = {\n",
    "    \"class_names\": class_names,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"train_size\": len(train_paths),\n",
    "    \"val_size\": len(val_paths),\n",
    "    \"test_size\": len(test_paths),\n",
    "}\n",
    "print(\"Pipeline ready:\", PIPELINE_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvqalC0pRjNZ"
   },
   "outputs": [],
   "source": [
    "# ==== 1.3A — Text preprocessing pipeline (descriptions) ====\n",
    "# What this cell does:\n",
    "# - Locates and loads waste_descriptions.csv\n",
    "# - Cleans text (light, reproducible)\n",
    "# - Tokenization via scikit-learn vectorizers\n",
    "# - Stratified train/test split\n",
    "# - Feature creation (TF-IDF word + character n-grams)\n",
    "# - Saves artifacts for later modeling\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ---------- Locate dataset ----------\n",
    "CANDIDATES = Path(\"waste_descriptions.csv\")\n",
    "\n",
    "df = pd.read_csv(desc_path)\n",
    "print(\"Rows:\", len(df), \"| Columns:\", list(df.columns))\n",
    "\n",
    "# ---------- Detect columns ----------\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want or want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col = pick(df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "label_col = pick(df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "assert text_col and label_col, \"Need at least a text column (description) and a label column (category).\"\n",
    "\n",
    "# ---------- Cleaning ----------\n",
    "def clean_text(s: str) -> str:\n",
    "    # Light, model-agnostic normalization. Do NOT over-clean (keep domain words).\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)            # collapse whitespace\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "df[text_col]  = df[text_col].astype(str).map(clean_text)\n",
    "df[label_col] = df[label_col].astype(str).str.strip()\n",
    "\n",
    "# Optional: drop empty descriptions after cleaning\n",
    "df = df[df[text_col].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# ---------- Stratified split ----------\n",
    "X = df[text_col].values\n",
    "y = df[label_col].values\n",
    "le = LabelEncoder().fit(y)\n",
    "y_idx = le.transform(y)\n",
    "labels = le.classes_.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_idx, test_size=0.2, random_state=42, stratify=y_idx\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)} | Test size: {len(X_test)} | Classes: {len(labels)}\")\n",
    "\n",
    "# ---------- Features: TF-IDF ----------\n",
    "# Word-level TF-IDF (1–2 grams) + char-level TF-IDF (useful for OCR/noisy tokens)\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_df=0.9\n",
    ")\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit on TRAIN only, transform Train/Test\n",
    "Xw_train = word_tfidf.fit_transform(X_train)\n",
    "Xw_test  = word_tfidf.transform(X_test)\n",
    "\n",
    "Xc_train = char_tfidf.fit_transform(X_train)\n",
    "Xc_test  = char_tfidf.transform(X_test)\n",
    "\n",
    "# Concatenate sparse features (word + char)\n",
    "from scipy.sparse import hstack\n",
    "X_train_feats = hstack([Xw_train, Xc_train]).tocsr()\n",
    "X_test_feats  = hstack([Xw_test,  Xc_test]).tocsr()\n",
    "\n",
    "print(\"Feature shapes:\", X_train_feats.shape, X_test_feats.shape)\n",
    "\n",
    "# ---------- Save artifacts ----------\n",
    "ART_DIR = Path(\"artifacts/text_features\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(word_tfidf, ART_DIR / \"word_tfidf.pkl\")\n",
    "joblib.dump(char_tfidf, ART_DIR / \"char_tfidf.pkl\")\n",
    "joblib.dump((X_train_feats, y_train), ART_DIR / \"train_features.pkl\")\n",
    "joblib.dump((X_test_feats,  y_test),  ART_DIR / \"test_features.pkl\")\n",
    "\n",
    "with open(ART_DIR / \"label_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"labels\": labels}, f, indent=2)\n",
    "\n",
    "print(\"Saved to:\", ART_DIR.resolve())\n",
    "\n",
    "# ---------- Tiny sanity check ----------\n",
    "print(\"Class distribution (train):\")\n",
    "display(pd.Series(y_train).value_counts().sort_index().rename(index=dict(enumerate(labels))))\n",
    "print(\"Class distribution (test):\")\n",
    "display(pd.Series(y_test).value_counts().sort_index().rename(index=dict(enumerate(labels))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLuYXuxARjNZ"
   },
   "outputs": [],
   "source": [
    "# ==== 1.3B — RAG document prep (policies) ====\n",
    "# What this cell does:\n",
    "# - Loads policy documents from JSON or CSV\n",
    "# - Splits each document into overlapping chunks with metadata\n",
    "# - Creates embeddings for retrieval:\n",
    "#     * Preferred: Sentence-Transformers (\"all-MiniLM-L6-v2\") dense embeddings + FAISS index\n",
    "#     * Fallback: TF-IDF sparse embeddings + cosine similarity (sklearn)\n",
    "# - Saves artifacts and provides a cached, case-insensitive quick search function\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ---------- Locate policy file ----------\n",
    "JSON_CAND = [\n",
    "    Path(\"/mnt/data/waste_policy_documents.json\"),\n",
    "    Path(\"data/waste_policy_documents.json\"),\n",
    "    Path(\"waste_policy_documents.json\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_policy_documents.json\",\n",
    "]\n",
    "CSV_CAND = [\n",
    "    Path(\"/mnt/data/waste_policy_documents.csv\"),\n",
    "    Path(\"data/waste_policy_documents.csv\"),\n",
    "    Path(\"waste_policy_documents.csv\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_policy_documents.csv\",\n",
    "]\n",
    "pol_json = next((p for p in JSON_CAND if p.exists()), None)\n",
    "pol_csv  = next((p for p in CSV_CAND if p.exists()), None)\n",
    "assert pol_json or pol_csv, \"Policy docs not found — add JSON or CSV and re-run.\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "if pol_json:\n",
    "    with open(pol_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    pdf = pd.json_normalize(data)\n",
    "else:\n",
    "    pdf = pd.read_csv(pol_csv)\n",
    "\n",
    "# ---------- Detect columns ----------\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    # exact first\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want:\n",
    "                return orig\n",
    "    # substring next\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col = pick(pdf.columns, [\"document_text\",\"text\",\"body\",\"content\"])\n",
    "cats_col = pick(pdf.columns, [\"categories_covered\",\"category\",\"categories\"])\n",
    "jur_col  = pick(pdf.columns, [\"jurisdiction\",\"city\",\"region\"])\n",
    "type_col = pick(pdf.columns, [\"policy_type\",\"type\"])\n",
    "pid_col  = pick(pdf.columns, [\"policy_id\",\"id\"])\n",
    "\n",
    "assert text_col, \"Need a text column for the policy content.\"\n",
    "if pid_col is None:\n",
    "    pdf[\"_pid_auto\"] = np.arange(len(pdf))\n",
    "    pid_col = \"_pid_auto\"\n",
    "\n",
    "# ---------- Cleaning + chunking ----------\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def to_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    if isinstance(x, str) and x.strip().startswith(\"[\"):\n",
    "        try:\n",
    "            arr = json.loads(x)\n",
    "            if isinstance(arr, list): return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [x] if x is not None else []\n",
    "\n",
    "def chunk_text(text, target_chars=520, overlap=100):\n",
    "    txt = clean_text(text)\n",
    "    sections = re.split(r\"\\n\\s*\\n\", txt)\n",
    "    chunks, buf = [], \"\"\n",
    "    for sec in sections:\n",
    "        if len(buf) + len(sec) + 1 <= target_chars:\n",
    "            buf = (buf + \"\\n\" + sec).strip()\n",
    "        else:\n",
    "            if buf: chunks.append(buf)\n",
    "            if len(sec) > target_chars:\n",
    "                start = 0\n",
    "                while start < len(sec):\n",
    "                    end = min(len(sec), start + target_chars)\n",
    "                    piece = sec[start:end]\n",
    "                    chunks.append(piece.strip())\n",
    "                    start = max(end - overlap, start + 1)\n",
    "            else:\n",
    "                buf = sec\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    # drop tiny fragments\n",
    "    return [c for c in chunks if len(c.split()) >= 12]\n",
    "\n",
    "rows = []\n",
    "for _, r in pdf.iterrows():\n",
    "    pid = r[pid_col]\n",
    "    ptype = r[type_col] if type_col else None\n",
    "    juri  = r[jur_col]  if jur_col  else None\n",
    "    cats  = to_list(r[cats_col]) if cats_col else []\n",
    "    text  = r[text_col] if pd.notna(r[text_col]) else \"\"\n",
    "    parts = chunk_text(text, target_chars=520, overlap=100)\n",
    "    if not parts:\n",
    "        continue\n",
    "    cats = cats if isinstance(cats, list) else [cats]\n",
    "    cats = cats or [None]\n",
    "    for cat in cats:\n",
    "        for i, ch in enumerate(parts):\n",
    "            rows.append({\n",
    "                \"policy_id\": pid,\n",
    "                \"policy_type\": ptype,\n",
    "                \"category\": cat,\n",
    "                \"jurisdiction\": juri,\n",
    "                \"chunk_ix\": i,\n",
    "                \"text\": ch\n",
    "            })\n",
    "\n",
    "chunks_df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "print(\"Chunks:\", len(chunks_df))\n",
    "display(chunks_df.head(10))\n",
    "\n",
    "# ---------- Embeddings & index ----------\n",
    "ART_DIR = Path(\"artifacts/rag\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Preferred: dense embeddings + FAISS; else TF-IDF fallback\n",
    "use_dense = False\n",
    "dense_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    use_dense = True\n",
    "except Exception as e:\n",
    "    print(\"[Info] Falling back to TF-IDF (Sentence-Transformers/FAISS not available):\", e)\n",
    "\n",
    "# global caches for fast repeated queries\n",
    "_DENSE_MODEL = None\n",
    "_DENSE_INDEX = None\n",
    "_TFIDF_BUNDLE = None\n",
    "_META_DF = None\n",
    "\n",
    "if use_dense:\n",
    "    model = SentenceTransformer(dense_model_name)\n",
    "    texts = chunks_df[\"text\"].tolist()\n",
    "    emb = model.encode(texts, batch_size=64, show_progress_bar=False,\n",
    "                       convert_to_numpy=True, normalize_embeddings=True)\n",
    "    dim = emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)           # cosine if embeddings are normalized\n",
    "    index.add(emb.astype(np.float32))\n",
    "    faiss.write_index(index, str(ART_DIR / \"faiss_index.ip\"))\n",
    "    chunks_df.to_csv(ART_DIR / \"policy_chunks.csv\", index=False)\n",
    "    joblib.dump({\"model_name\": dense_model_name}, ART_DIR / \"dense_meta.pkl\")\n",
    "    print(\"Saved dense FAISS index + meta to:\", ART_DIR.resolve())\n",
    "else:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                          ngram_range=(1,2), min_df=1, max_df=1.0)\n",
    "    X = vec.fit_transform(chunks_df[\"text\"].fillna(\"\"))\n",
    "    chunks_df.to_csv(ART_DIR / \"policy_chunks.csv\", index=False)\n",
    "    joblib.dump({\"vectorizer\": vec, \"matrix\": X, \"meta\": chunks_df}, ART_DIR / \"tfidf_index.pkl\")\n",
    "    print(\"Saved TF-IDF index to:\", ART_DIR.resolve())\n",
    "\n",
    "def _norm(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, float) and np.isnan(s):\n",
    "        return None\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "# ---------- Quick search helper (cached, case-insensitive filters) ----------\n",
    "def search_policies(query, top_k=5, category=None, jurisdiction=None):\n",
    "    \"\"\"\n",
    "    Search policy chunks by query text. Optional filters:\n",
    "    - category: case-insensitive exact match on chunk 'category'\n",
    "    - jurisdiction: case-insensitive exact match on chunk 'jurisdiction'\n",
    "    \"\"\"\n",
    "    global _DENSE_MODEL, _DENSE_INDEX, _TFIDF_BUNDLE, _META_DF\n",
    "\n",
    "    cat_norm = _norm(category)\n",
    "    jur_norm = _norm(jurisdiction)\n",
    "\n",
    "    if _META_DF is None:\n",
    "        _META_DF = pd.read_csv(ART_DIR / \"policy_chunks.csv\")\n",
    "        if \"category\" in _META_DF.columns:\n",
    "            _META_DF[\"_cat_norm\"] = _META_DF[\"category\"].astype(str).str.lower()\n",
    "        else:\n",
    "            _META_DF[\"_cat_norm\"] = \"\"\n",
    "        if \"jurisdiction\" in _META_DF.columns:\n",
    "            _META_DF[\"_jur_norm\"] = _META_DF[\"jurisdiction\"].astype(str).str.lower()\n",
    "        else:\n",
    "            _META_DF[\"_jur_norm\"] = \"\"\n",
    "\n",
    "    if use_dense:\n",
    "        if _DENSE_INDEX is None:\n",
    "            _DENSE_INDEX = faiss.read_index(str(ART_DIR / \"faiss_index.ip\"))\n",
    "        if _DENSE_MODEL is None:\n",
    "            _DENSE_MODEL = SentenceTransformer(dense_model_name)\n",
    "\n",
    "        q = _DENSE_MODEL.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "        D, I = _DENSE_INDEX.search(q, top_k*10)  # oversample, filter later\n",
    "        I = I[0]; D = D[0]\n",
    "\n",
    "        rows = []\n",
    "        for i, score in zip(I, D):\n",
    "            row = _META_DF.iloc[int(i)]\n",
    "            if cat_norm and row[\"_cat_norm\"] != cat_norm:\n",
    "                continue\n",
    "            if jur_norm and row[\"_jur_norm\"] != jur_norm:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"score\": float(score),\n",
    "                \"policy_id\": row[\"policy_id\"],\n",
    "                \"category\": row.get(\"category\"),\n",
    "                \"jurisdiction\": row.get(\"jurisdiction\"),\n",
    "                \"text\": row[\"text\"][:260] + (\"...\" if len(row[\"text\"])>260 else \"\")\n",
    "            })\n",
    "            if len(rows) >= top_k:\n",
    "                break\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # TF-IDF fallback (cosine similarity)\n",
    "    if _TFIDF_BUNDLE is None:\n",
    "        _TFIDF_BUNDLE = joblib.load(ART_DIR / \"tfidf_index.pkl\")\n",
    "    vec, X, meta = _TFIDF_BUNDLE[\"vectorizer\"], _TFIDF_BUNDLE[\"matrix\"], _TFIDF_BUNDLE[\"meta\"]\n",
    "    if \"_cat_norm\" not in meta.columns or \"_jur_norm\" not in meta.columns:\n",
    "        meta = meta.copy()\n",
    "        meta[\"_cat_norm\"] = meta[\"category\"].astype(str).str.lower()\n",
    "        meta[\"_jur_norm\"] = meta[\"jurisdiction\"].astype(str).str.lower()\n",
    "        _TFIDF_BUNDLE[\"meta\"] = meta  # cache back\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    qv = vec.transform([query])\n",
    "    sims = cosine_similarity(qv, X).ravel()\n",
    "    idx = np.argsort(-sims)\n",
    "\n",
    "    rows, taken = [], 0\n",
    "    for i in idx:\n",
    "        row = meta.iloc[int(i)]\n",
    "        if cat_norm and row[\"_cat_norm\"] != cat_norm:\n",
    "            continue\n",
    "        if jur_norm and row[\"_jur_norm\"] != jur_norm:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"score\": float(sims[i]),\n",
    "            \"policy_id\": row[\"policy_id\"],\n",
    "            \"category\": row.get(\"category\"),\n",
    "            \"jurisdiction\": row.get(\"jurisdiction\"),\n",
    "            \"text\": row[\"text\"][:260] + (\"...\" if len(row[\"text\"])>260 else \"\")\n",
    "        })\n",
    "        taken += 1\n",
    "        if taken >= top_k:\n",
    "            break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Demo query ----------\n",
    "demo = search_policies(\"Rinse plastic bottle, remove cap | Metro City\", top_k=5,\n",
    "                       category=\"Plastic\", jurisdiction=\"Metro City\")\n",
    "display(demo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcnO9HqoRjNZ"
   },
   "source": [
    "## Part 2: Waste Material Classification with CNN\n",
    "\n",
    "In this section, you will build a CNN model to classify waste materials from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGuSWFkYRjNZ"
   },
   "source": [
    "### 2.1 Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHw7IWbNRjNZ"
   },
   "outputs": [],
   "source": [
    "# ==== 2.1 — Preprocess Images (augmentation + normalization) ====\n",
    "# - Reuses existing tf.data datasets if defined; otherwise builds from a directory\n",
    "# - Augments training batches only\n",
    "# - Normalizes correctly for the chosen backbone (ResNet/MobileNet/EfficientNet)\n",
    "# - Exposes: train_ds, validation_ds, test_ds, class_names, BACKBONE, IMG_HEIGHT, IMG_WIDTH\n",
    "\n",
    "import pathlib, os, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "data_dir = pathlib.Path(\"RealWaste\")  # folder that directly contains the class subfolders\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH  = 224\n",
    "BACKBONE   = \"resnet50\"  # one of: \"resnet50\", \"mobilenet_v2\", \"efficientnet_b0\"\n",
    "# ----------------------------\n",
    "\n",
    "def get_preprocess(backbone: str):\n",
    "    b = backbone.lower()\n",
    "    if b == \"resnet50\":\n",
    "        from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    elif b == \"mobilenet_v2\":\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "    elif b == \"efficientnet_b0\":\n",
    "        from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported BACKBONE. Choose resnet50 | mobilenet_v2 | efficientnet_b0\")\n",
    "    return preprocess_input\n",
    "\n",
    "preprocess_input = get_preprocess(BACKBONE)\n",
    "\n",
    "# Light & safe augmentation (expects inputs in [0,1])\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.03),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.05),\n",
    "    ],\n",
    "    name=\"augment\",\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def apply_preprocessing(ds, training: bool):\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      raw -> resize -> float32\n",
    "         -> scale to [0,1] (auto-detect if already [0,1] vs [0,255])\n",
    "         -> augmentation (train only)\n",
    "         -> scale back to [0,255]\n",
    "         -> backbone preprocess_input\n",
    "    \"\"\"\n",
    "    def _map(img, y):\n",
    "        img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "        img = tf.cast(img, tf.float32)\n",
    "\n",
    "        # Auto-detect input range and convert to [0,1]\n",
    "        max_val = tf.reduce_max(img)\n",
    "        img01 = tf.cond(max_val > 1.5, lambda: img / 255.0, lambda: img)\n",
    "\n",
    "        if training:\n",
    "            img01 = data_augmentation(img01, training=True)\n",
    "\n",
    "        # Back to [0,255] before preprocess_input\n",
    "        img255 = img01 * 255.0\n",
    "        img_pp = preprocess_input(img255)\n",
    "        return img_pp, y\n",
    "\n",
    "    return ds.map(_map, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "def build_datasets_if_needed():\n",
    "    g = globals()\n",
    "    # Reuse if already built\n",
    "    if all(k in g for k in [\"train_ds\", \"validation_ds\", \"test_ds\"]):\n",
    "        return g[\"train_ds\"], g[\"validation_ds\"], g[\"test_ds\"], g.get(\"class_names\", None)\n",
    "\n",
    "    # Otherwise build from directory (80/10/10 via val split halving)\n",
    "    assert data_dir.exists(), f\"Directory not found: {data_dir}\"\n",
    "    train_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "    )\n",
    "    # Split validation into val/test equally\n",
    "    val_batches = tf.data.experimental.cardinality(val_raw)\n",
    "    test_raw = val_raw.take(val_batches // 2)\n",
    "    val_raw  = val_raw.skip(val_batches // 2)\n",
    "\n",
    "    # Cache raw (I/O bound) before heavy mapping\n",
    "    train_raw = train_raw.cache()\n",
    "    val_raw   = val_raw.cache()\n",
    "    test_raw  = test_raw.cache()\n",
    "\n",
    "    # Apply preprocessing\n",
    "    train_ds      = apply_preprocessing(train_raw, training=True)\n",
    "    validation_ds = apply_preprocessing(val_raw,   training=False)\n",
    "    test_ds       = apply_preprocessing(test_raw,  training=False)\n",
    "\n",
    "    # Class names (from directory dataset)\n",
    "    class_names = getattr(train_raw, \"class_names\", None)\n",
    "    return train_ds, validation_ds, test_ds, class_names\n",
    "\n",
    "# Build or reuse datasets\n",
    "train_ds, validation_ds, test_ds, class_names = build_datasets_if_needed()\n",
    "\n",
    "# Introspection\n",
    "print(\"Batches ->\",\n",
    "      \"train:\",       tf.data.experimental.cardinality(train_ds).numpy(),\n",
    "      \"| validation:\", tf.data.experimental.cardinality(validation_ds).numpy(),\n",
    "      \"| test:\",       tf.data.experimental.cardinality(test_ds).numpy())\n",
    "\n",
    "# Quick sanity check (range & shapes)\n",
    "for imgs, ys in train_ds.take(1):\n",
    "    print(\"train batch:\", imgs.shape, ys.shape, imgs.dtype, ys.dtype)\n",
    "    print(\"min/max:\", float(tf.reduce_min(imgs).numpy()), float(tf.reduce_max(imgs).numpy()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atg_s1c5RjNa"
   },
   "source": [
    "### 2.2 Implement CNN Model with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6HPFM1mRjNa"
   },
   "outputs": [],
   "source": [
    "# ==== 2.2 — CNN with Transfer Learning (TensorFlow/Keras) ====\n",
    "# - Uses the SAME BACKBONE as step 2.1 (to match preprocessing)\n",
    "# - Two-phase training: warmup (frozen base) + fine-tune (partial unfreeze)\n",
    "# - Tracks acc/prec/rec/F1; saves best checkpoint and plots curves\n",
    "# - Evaluates on test set and prints a labeled confusion matrix\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure datasets are available\n",
    "for name in [\"train_ds\", \"validation_ds\", \"test_ds\"]:\n",
    "    assert name in globals(), f\"{name} is not defined. Run step 2.1 first.\"\n",
    "\n",
    "# Auto-detect number of classes\n",
    "num_classes = int(train_ds.element_spec[1].shape[-1])\n",
    "print(\"Detected num_classes:\", num_classes)\n",
    "\n",
    "# Backbone factory (must match 2.1 BACKBONE)\n",
    "def make_backbone(name: str, input_shape):\n",
    "    name = name.lower()\n",
    "    if name == \"mobilenet_v2\":\n",
    "        from tensorflow.keras.applications import MobileNetV2\n",
    "        base = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif name == \"efficientnet_b0\":\n",
    "        from tensorflow.keras.applications import EfficientNetB0\n",
    "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif name == \"resnet50\":\n",
    "        from tensorflow.keras.applications import ResNet50\n",
    "        base = ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported BACKBONE. Choose mobilenet_v2 | efficientnet_b0 | resnet50\")\n",
    "    return base\n",
    "\n",
    "# Explicit 3-channel input shape (required by ImageNet weights)\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "# Sanity-check channels from the pipeline\n",
    "_sample_imgs, _ = next(iter(train_ds))\n",
    "assert _sample_imgs.shape[-1] == 3, (\n",
    "    f\"Expected 3-channel RGB images, got shape {tuple(_sample_imgs.shape)}.\"\n",
    ")\n",
    "\n",
    "base = make_backbone(BACKBONE, input_shape)\n",
    "base.trainable = False  # warmup\n",
    "\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = base(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "x = tf.keras.layers.Dropout(0.25, name=\"head_dropout\")(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"classifier\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=f\"{BACKBONE}_waste_classifier\")\n",
    "model.summary()\n",
    "\n",
    "# Metrics\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_labels = tf.argmax(y_true, axis=-1)\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)\n",
    "        self.precision.update_state(y_true_labels, y_pred_labels, sample_weight)\n",
    "        self.recall.update_state(y_true_labels, y_pred_labels, sample_weight)\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2.0 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "    tf.keras.metrics.Precision(name=\"prec\"),\n",
    "    tf.keras.metrics.Recall(name=\"rec\"),\n",
    "    F1Score(name=\"f1\"),\n",
    "]\n",
    "\n",
    "# Optional class weights (enable if imbalance exists)\n",
    "def compute_class_weights(ds, num_classes):\n",
    "    counts = np.zeros(num_classes, dtype=np.int64)\n",
    "    for _, y in ds.unbatch():\n",
    "        cls = int(tf.argmax(y, axis=-1).numpy())\n",
    "        counts[cls] += 1\n",
    "    total = counts.sum()\n",
    "    weights = total / (np.maximum(counts, 1) * num_classes)\n",
    "    return {i: float(w) for i, w in enumerate(weights)}\n",
    "\n",
    "# class_weights = compute_class_weights(train_ds, num_classes)\n",
    "class_weights = None\n",
    "\n",
    "# Callbacks\n",
    "CHECKPOINT_DIR = \"checkpoints/vision\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, f\"{BACKBONE}_best.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        ckpt_path, monitor=\"val_acc\", mode=\"max\",\n",
    "        save_best_only=True, save_weights_only=False, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", mode=\"max\", patience=5, restore_best_weights=True, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Phase 1 — warmup\n",
    "INIT_LR = 3e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=INIT_LR),\n",
    "              loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "history1 = model.fit(\n",
    "    train_ds, validation_data=validation_ds,\n",
    "    epochs=5, class_weight=class_weights, verbose=1\n",
    ")\n",
    "\n",
    "# Phase 2 — fine-tune tail (keep BatchNorm frozen)\n",
    "layers_to_unfreeze = {\"efficientnet_b0\": 80, \"mobilenet_v2\": 60, \"resnet50\": 50}\n",
    "tail = layers_to_unfreeze.get(BACKBONE.lower(), 50)\n",
    "for layer in base.layers[-tail:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "FT_LR = 1e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=FT_LR),\n",
    "              loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "history2 = model.fit(\n",
    "    train_ds, validation_data=validation_ds,\n",
    "    epochs=10, callbacks=callbacks, class_weight=class_weights, verbose=1\n",
    ")\n",
    "\n",
    "# Test evaluation\n",
    "test_metrics = model.evaluate(test_ds, verbose=1)\n",
    "print(\"Test metrics:\", dict(zip(model.metrics_names, test_metrics)))\n",
    "\n",
    "# Curves\n",
    "def plot_curves(h1, h2, key, title):\n",
    "    vals = (h1.history.get(key, []) + h2.history.get(key, []))\n",
    "    val_vals = (h1.history.get(\"val_\"+key, []) + h2.history.get(\"val_\"+key, []))\n",
    "    epochs = range(1, len(vals) + 1)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(epochs, vals, label=key)\n",
    "    plt.plot(epochs, val_vals, label=\"val_\"+key)\n",
    "    plt.title(title); plt.xlabel(\"Epoch\"); plt.ylabel(key)\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "for k, t in [(\"loss\",\"Loss\"), (\"acc\",\"Accuracy\"), (\"f1\",\"F1\")]:\n",
    "    if (k in history1.history or \"val_\"+k in history1.history) or (k in history2.history or \"val_\"+k in history2.history):\n",
    "        plot_curves(history1, history2, k, f\"Training curves — {t}\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_true, y_pred = [], []\n",
    "for imgs, ys in test_ds:\n",
    "    probs = model.predict(imgs, verbose=0)\n",
    "    y_pred.extend(np.argmax(probs, axis=-1).tolist())\n",
    "    y_true.extend(np.argmax(ys.numpy(), axis=-1).tolist())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "labels_for_plot = class_names if \"class_names\" in globals() and class_names is not None and len(class_names) == num_classes else list(range(num_classes))\n",
    "\n",
    "print(\"\\nClassification report:\\n\",\n",
    "      classification_report(y_true, y_pred,\n",
    "                            target_names=labels_for_plot if len(labels_for_plot)==num_classes else None,\n",
    "                            digits=3))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_for_plot)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Save final fine-tuned model\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "final_model_path = f\"artifacts/cnn_{BACKBONE}_finetuned.keras\"\n",
    "model.save(final_model_path)\n",
    "print(\"Saved model to:\", final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w55nRURdRjNa"
   },
   "source": [
    "### 2.3 Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6flDXl3RjNa"
   },
   "outputs": [],
   "source": [
    "# ==== 2.3A — Train the CNN (regularization + callbacks) ====\n",
    "# - Reuses an existing `model` if available; otherwise builds ImageNet-based classifier\n",
    "# - Regularization: dropout, L2 weight decay, label smoothing\n",
    "# - Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "# - Trains in warmup + finetune, plots curves, (optional) evaluates on test\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Ensure datasets exist\n",
    "for name in [\"train_ds\", \"validation_ds\", \"test_ds\"]:\n",
    "    assert name in globals(), f\"{name} is not defined. Run 2.1 first.\"\n",
    "\n",
    "# Auto-detect classes\n",
    "num_classes = int(train_ds.element_spec[1].shape[-1])\n",
    "print(\"Detected num_classes:\", num_classes)\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "# IMPORTANT: use the SAME BACKBONE as in 2.1; if it's already defined there, do not override here.\n",
    "# BACKBONE         = \"resnet50\"          # uncomment ONLY if not defined in 2.1\n",
    "DROPOUT_HEAD     = 0.30\n",
    "L2_WEIGHT_DECAY  = 1e-4\n",
    "LABEL_SMOOTHING  = 0.05\n",
    "WARMUP_EPOCHS    = 3\n",
    "FINETUNE_EPOCHS  = 10\n",
    "INIT_LR          = 3e-4\n",
    "FT_LR            = 1e-4\n",
    "CHECKPOINT_DIR   = \"checkpoints/vision_2_3\"\n",
    "LOG_PATH         = os.path.join(CHECKPOINT_DIR, \"train_log.csv\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Explicit input shape (three channels)\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "# Backbone factory\n",
    "def make_backbone(name: str, input_shape):\n",
    "    n = name.lower()\n",
    "    if n == \"mobilenet_v2\":\n",
    "        from tensorflow.keras.applications import MobileNetV2\n",
    "        return MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif n == \"efficientnet_b0\":\n",
    "        from tensorflow.keras.applications import EfficientNetB0\n",
    "        return EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif n == \"resnet50\":\n",
    "        from tensorflow.keras.applications import ResNet50\n",
    "        return ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone.\")\n",
    "\n",
    "# Build model if not provided\n",
    "def build_model(num_classes, backbone, dropout=0.3, l2=1e-4):\n",
    "    base = make_backbone(backbone, input_shape)\n",
    "    base.trainable = False  # warmup\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout, name=\"dropout\")(x)\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        num_classes, activation=\"softmax\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2),\n",
    "        name=\"classifier\"\n",
    "    )(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=f\"{backbone.lower()}_waste_cls\")\n",
    "    # keep a reference to base for finetuning\n",
    "    model.base = base\n",
    "    return model\n",
    "\n",
    "# F1 metric\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        yt = tf.argmax(y_true, axis=-1)\n",
    "        yp = tf.argmax(y_pred, axis=-1)\n",
    "        self.precision.update_state(yt, yp, sample_weight)\n",
    "        self.recall.update_state(yt, yp, sample_weight)\n",
    "    def result(self):\n",
    "        p, r = self.precision.result(), self.recall.result()\n",
    "        return 2.0 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "# Reuse existing model if present; else build with the SAME BACKBONE as 2.1\n",
    "if \"model\" not in globals():\n",
    "    assert \"BACKBONE\" in globals(), \"BACKBONE must match step 2.1.\"\n",
    "    model = build_model(num_classes, backbone=BACKBONE, dropout=DROPOUT_HEAD, l2=L2_WEIGHT_DECAY)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, f\"{model.name}_best.keras\")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_acc\", mode=\"max\",\n",
    "                                       save_best_only=True, save_weights_only=False, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
    "    tf.keras.callbacks.CSVLogger(LOG_PATH, append=True),\n",
    "]\n",
    "\n",
    "# Phase 1 — warmup\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=INIT_LR),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "             tf.keras.metrics.Precision(name=\"prec\"),\n",
    "             tf.keras.metrics.Recall(name=\"rec\"),\n",
    "             F1Score(name=\"f1\")]\n",
    ")\n",
    "hist1 = model.fit(train_ds, validation_data=validation_ds, epochs=WARMUP_EPOCHS, verbose=1)\n",
    "\n",
    "# Phase 2 — finetune (unfreeze a sensible tail; keep BN frozen)\n",
    "tail_by_backbone = {\"efficientnet_b0\": 80, \"mobilenet_v2\": 60, \"resnet50\": 50}\n",
    "tail = tail_by_backbone.get(BACKBONE.lower(), 50)\n",
    "for layer in getattr(model, \"base\", model.layers[1]).layers[-tail:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=FT_LR),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "             tf.keras.metrics.Precision(name=\"prec\"),\n",
    "             tf.keras.metrics.Recall(name=\"rec\"),\n",
    "             F1Score(name=\"f1\")]\n",
    ")\n",
    "hist2 = model.fit(train_ds, validation_data=validation_ds,\n",
    "                  epochs=FINETUNE_EPOCHS, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_curves(h1, h2, key, title):\n",
    "    vals = (h1.history.get(key, []) + h2.history.get(key, []))\n",
    "    val_vals = (h1.history.get(\"val_\"+key, []) + h2.history.get(\"val_\"+key, []))\n",
    "    if not vals and not val_vals:\n",
    "        return\n",
    "    epochs = range(1, len(vals) + 1)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(epochs, vals, label=key)\n",
    "    plt.plot(epochs, val_vals, label=\"val_\"+key)\n",
    "    plt.title(title); plt.xlabel(\"Epoch\"); plt.ylabel(key)\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "for k, t in [(\"loss\",\"Loss\"), (\"acc\",\"Accuracy\"), (\"f1\",\"F1\")]:\n",
    "    plot_curves(hist1, hist2, k, f\"Training curves — {t}\")\n",
    "\n",
    "print(\"Checkpoint saved to:\", ckpt_path)\n",
    "\n",
    "# (Optional) quick test evaluation and save final model\n",
    "test_metrics = model.evaluate(test_ds, verbose=1)\n",
    "print(\"Test metrics:\", dict(zip(model.metrics_names, test_metrics)))\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "final_path = f\"artifacts/{model.name}_finetuned.keras\"\n",
    "model.save(final_path)\n",
    "print(\"Saved model to:\", final_path)\n",
    "\n",
    "# (Optional) quick classification report\n",
    "y_true, y_pred = [], []\n",
    "for xb, yb in test_ds:\n",
    "    probs = model.predict(xb, verbose=0)\n",
    "    y_pred.extend(np.argmax(probs, axis=1))\n",
    "    y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2mXbdmGRjNa"
   },
   "outputs": [],
   "source": [
    "# ==== 2.3B — Evaluate model performance: accuracy, confusion matrix, error analysis ====\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "assert \"model\" in globals(), \"Model is not defined. Run 2.3A first.\"\n",
    "for name in [\"test_ds\"]:\n",
    "    assert name in globals(), f\"{name} is not defined. Run 2.1 first.\"\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = model.evaluate(test_ds, verbose=1)\n",
    "print(\"Test metrics:\", dict(zip(model.metrics_names, test_metrics)))\n",
    "\n",
    "# Collect predictions and true labels\n",
    "y_true, y_pred = [], []\n",
    "for imgs, ys in test_ds:\n",
    "    probs = model.predict(imgs, verbose=0)\n",
    "    y_pred.extend(np.argmax(probs, axis=-1))\n",
    "    y_true.extend(np.argmax(ys.numpy(), axis=-1))\n",
    "y_true = np.array(y_true, dtype=int)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "num_classes = int(max(y_true.max(), y_pred.max()) + 1)\n",
    "\n",
    "# Use human-readable class names if available\n",
    "labels_for_plot = class_names if \"class_names\" in globals() and class_names is not None and len(class_names) == num_classes else list(range(num_classes))\n",
    "\n",
    "# Confusion matrix (counts)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "\n",
    "# Pretty plot\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_for_plot)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix (counts)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Row-normalized confusion (per-class accuracy visualization)\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "cm_row = cm / np.clip(row_sums, 1, None)\n",
    "\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_row, display_labels=labels_for_plot)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp_norm.plot(ax=ax, xticks_rotation=45, cmap=\"Greens\", colorbar=True, values_format=\".2f\")\n",
    "plt.title(\"Confusion Matrix (row-normalized)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Per-class report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=labels_for_plot if len(labels_for_plot) == num_classes else None,\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Per-class accuracy (from row-normalized CM diagonal)\n",
    "per_class_acc = (cm.diagonal() / np.maximum(row_sums.squeeze(), 1)).round(3)\n",
    "print(\"Per-class accuracy:\", per_class_acc.tolist())\n",
    "\n",
    "# Top confusion pairs (true -> predicted : count)\n",
    "cm_off = cm.copy()\n",
    "np.fill_diagonal(cm_off, 0)\n",
    "pairs = [((i, j), int(cm_off[i, j])) for i in range(num_classes) for j in range(num_classes) if cm_off[i, j] > 0]\n",
    "pairs = sorted(pairs, key=lambda x: -x[1])[:10]\n",
    "print(\"\\nTop confusion pairs (true -> predicted : count)\")\n",
    "for (i, j), c in pairs:\n",
    "    ti = labels_for_plot[i] if i < len(labels_for_plot) else i\n",
    "    tj = labels_for_plot[j] if j < len(labels_for_plot) else j\n",
    "    print(f\"{ti} -> {tj} : {c}\")\n",
    "\n",
    "# Save metrics to disk for your report\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "out_json = {\n",
    "    \"metrics_names\": model.metrics_names,\n",
    "    \"metrics_values\": [float(x) for x in test_metrics],\n",
    "    \"per_class_accuracy\": per_class_acc.tolist(),\n",
    "    \"labels\": list(labels_for_plot) if isinstance(labels_for_plot, list) else labels_for_plot,\n",
    "    \"confusion_pairs_top\": [\n",
    "        {\"true_index\": int(i), \"true_label\": (labels_for_plot[i] if i < len(labels_for_plot) else i),\n",
    "         \"pred_index\": int(j), \"pred_label\": (labels_for_plot[j] if j < len(labels_for_plot) else j),\n",
    "         \"count\": int(c)}\n",
    "        for (i, j), c in pairs\n",
    "    ],\n",
    "}\n",
    "with open(\"reports/test_metrics_2_3.json\", \"w\") as f:\n",
    "    json.dump(out_json, f, indent=2)\n",
    "print(\"Saved evaluation to reports/test_metrics_2_3.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e93UA8VvRjNa"
   },
   "source": [
    "### 2.4 Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2.4 — Fine-tune the Model (random search over hyperparameters) ====\n",
    "# What this cell does:\n",
    "# - Reuses train_ds / validation_ds / test_ds from earlier cells\n",
    "# - Randomly samples a few hyperparameter combos (edit N_TRIALS to widen or narrow)\n",
    "# - For each trial: build model -> warmup (frozen base) -> fine-tune (top layers unfrozen)\n",
    "# - Tracks metrics, keeps best trial by val accuracy, saves artifacts, evaluates on test set\n",
    "# NOTE: The backbone MUST match step 2.1 preprocessing (we lock to BACKBONE from 2.1).\n",
    "\n",
    "import os, json, itertools, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# -------- prerequisites --------\n",
    "for name in [\"train_ds\", \"validation_ds\", \"test_ds\", \"BACKBONE\", \"IMG_HEIGHT\", \"IMG_WIDTH\"]:\n",
    "    assert name in globals(), f\"{name} is not defined. Run steps 2.1/2.3 first.\"\n",
    "\n",
    "num_classes = int(train_ds.element_spec[1].shape[-1])\n",
    "print(\"Detected num_classes:\", num_classes)\n",
    "\n",
    "# Lock backbone to the SAME one used in step 2.1\n",
    "BACKBONES = [BACKBONE]  # do not change unless you also change preprocessing\n",
    "\n",
    "# Input shape must be RGB (3 channels)\n",
    "example_batch = next(iter(train_ds))\n",
    "input_shape = tuple(example_batch[0].shape[1:])\n",
    "assert input_shape[-1] == 3, f\"Expected 3-channel RGB, got {input_shape}\"\n",
    "\n",
    "# -------- search space (edit as needed) --------\n",
    "LRS_INIT           = [3e-4, 1e-4]\n",
    "LRS_FT             = [1e-4, 5e-5]\n",
    "DROPOUTS           = [0.25, 0.35]\n",
    "L2_WEIGHTS         = [1e-5, 1e-4]\n",
    "LABEL_SMOOTHINGS   = [0.0, 0.05]\n",
    "TOP_UNFREEZE       = [50, 80]                 # how many tail layers to unfreeze (non-BN)\n",
    "WARMUP_EPOCHS      = 3\n",
    "FINETUNE_EPOCHS    = 6\n",
    "N_TRIALS           = 8\n",
    "\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints/vision_2_4\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# -------- helper: custom F1 metric --------\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        yt = tf.argmax(y_true, axis=-1)\n",
    "        yp = tf.argmax(y_pred, axis=-1)\n",
    "        self.precision.update_state(yt, yp, sample_weight)\n",
    "        self.recall.update_state(yt, yp, sample_weight)\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2.0 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "# -------- backbone factory (MUST match preprocessing from 2.1) --------\n",
    "def make_backbone(name: str, input_shape):\n",
    "    name = name.lower()\n",
    "    if name == \"mobilenet_v2\":\n",
    "        from tensorflow.keras.applications import MobileNetV2\n",
    "        base = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif name == \"efficientnet_b0\":\n",
    "        from tensorflow.keras.applications import EfficientNetB0\n",
    "        base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif name == \"resnet50\":\n",
    "        from tensorflow.keras.applications import ResNet50\n",
    "        base = ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone.\")\n",
    "    return base\n",
    "\n",
    "def build_model(backbone_name, dropout, l2, input_shape, num_classes):\n",
    "    base = make_backbone(backbone_name, input_shape)\n",
    "    base.trainable = False\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout, name=\"dropout\")(x)\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        num_classes, activation=\"softmax\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2),\n",
    "        name=\"classifier\"\n",
    "    )(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=f\"{backbone_name}_cls\")\n",
    "    return model, base\n",
    "\n",
    "def unfreeze_tail_layers(base_model, n_to_unfreeze):\n",
    "    \"\"\"Unfreeze last N non-BN layers (keeps BatchNorm frozen).\"\"\"\n",
    "    count = 0\n",
    "    for layer in reversed(base_model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            continue\n",
    "        layer.trainable = True\n",
    "        count += 1\n",
    "        if count >= n_to_unfreeze:\n",
    "            break\n",
    "\n",
    "def train_one_trial(cfg, input_shape):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model, base = build_model(cfg[\"backbone\"], cfg[\"dropout\"], cfg[\"l2\"], input_shape, num_classes)\n",
    "\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "               tf.keras.metrics.Precision(name=\"prec\"),\n",
    "               tf.keras.metrics.Recall(name=\"rec\"),\n",
    "               F1Score(name=\"f1\")]\n",
    "\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, f\"best_{cfg['trial_id']}.keras\")\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_acc\", mode=\"max\",\n",
    "                                           save_best_only=True, save_weights_only=False, verbose=0),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=3, restore_best_weights=True, verbose=0),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=0),\n",
    "    ]\n",
    "\n",
    "    # Phase 1: warmup\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=cfg[\"lr_init\"]),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=cfg[\"label_smoothing\"]),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    model.fit(train_ds, validation_data=validation_ds, epochs=WARMUP_EPOCHS, verbose=0)\n",
    "\n",
    "    # Phase 2: fine-tune\n",
    "    unfreeze_tail_layers(base, cfg[\"top_unfreeze\"])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=cfg[\"lr_ft\"]),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=cfg[\"label_smoothing\"]),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    _ = model.fit(train_ds, validation_data=validation_ds, epochs=FINETUNE_EPOCHS, callbacks=callbacks, verbose=0)\n",
    "\n",
    "    # Evaluate on validation\n",
    "    val_metrics = model.evaluate(validation_ds, verbose=0)\n",
    "    result = dict(zip(model.metrics_names, [float(x) for x in val_metrics]))\n",
    "    result[\"ckpt_path\"] = ckpt_path\n",
    "    return result\n",
    "\n",
    "# Build a list of candidate configs, then randomly sample N_TRIALS\n",
    "all_combos = list(itertools.product(BACKBONES, LRS_INIT, LRS_FT, DROPOUTS, L2_WEIGHTS, LABEL_SMOOTHINGS, TOP_UNFREEZE))\n",
    "random.shuffle(all_combos)\n",
    "candidates = all_combos[:N_TRIALS]\n",
    "\n",
    "trial_cfgs = []\n",
    "for t, (bb, lr0, lrf, dr, l2w, ls, unf) in enumerate(candidates):\n",
    "    trial_cfgs.append({\n",
    "        \"trial_id\": f\"t{t:02d}\",\n",
    "        \"backbone\": bb,\n",
    "        \"lr_init\": lr0,\n",
    "        \"lr_ft\": lrf,\n",
    "        \"dropout\": dr,\n",
    "        \"l2\": l2w,\n",
    "        \"label_smoothing\": ls,\n",
    "        \"top_unfreeze\": unf,\n",
    "    })\n",
    "\n",
    "print(\"Running trials:\")\n",
    "for c in trial_cfgs:\n",
    "    print(c)\n",
    "\n",
    "# Run trials\n",
    "sweep_results = []\n",
    "best = None\n",
    "for cfg in trial_cfgs:\n",
    "    res = train_one_trial(cfg, input_shape)\n",
    "    rec = {\"cfg\": cfg, \"val_metrics\": res}\n",
    "    sweep_results.append(rec)\n",
    "    score = res.get(\"acc\", 0.0)  # primary selection metric\n",
    "    if (best is None) or (score > best[\"score\"]):\n",
    "        best = {\"score\": score, \"cfg\": cfg, \"ckpt\": res[\"ckpt_path\"], \"val\": res}\n",
    "\n",
    "# Guards\n",
    "if not sweep_results:\n",
    "    raise RuntimeError(\"No trials ran — check your search space or datasets.\")\n",
    "if best is None:\n",
    "    raise RuntimeError(\"No best trial selected — check metric names or training logs.\")\n",
    "\n",
    "# Save sweep results\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "with open(\"reports/vision_2_4_sweep.json\", \"w\") as f:\n",
    "    json.dump(sweep_results, f, indent=2)\n",
    "print(\"\\nBest trial by val_acc:\", best)\n",
    "\n",
    "# -------- load best model, evaluate on test set --------\n",
    "best_model = tf.keras.models.load_model(best[\"ckpt\"], compile=False)\n",
    "best_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "             tf.keras.metrics.Precision(name=\"prec\"),\n",
    "             tf.keras.metrics.Recall(name=\"rec\"),\n",
    "             F1Score(name=\"f1\")]\n",
    ")\n",
    "test_metrics = best_model.evaluate(test_ds, verbose=1)\n",
    "print(\"\\nTest metrics (best model):\", dict(zip(best_model.metrics_names, [float(x) for x in test_metrics])))\n",
    "\n",
    "# Confusion matrix + labeled report\n",
    "y_true, y_pred = [], []\n",
    "for imgs, ys in test_ds:\n",
    "    probs = best_model.predict(imgs, verbose=0)\n",
    "    y_pred.extend(np.argmax(probs, axis=-1))\n",
    "    y_true.extend(np.argmax(ys.numpy(), axis=-1))\n",
    "y_true = np.array(y_true, dtype=int)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels_for_plot = class_names if \"class_names\" in globals() and class_names is not None and len(class_names) == cm.shape[0] else list(range(cm.shape[0]))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_for_plot)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix — Best Trial\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\\nClassification report:\\n\",\n",
    "      classification_report(y_true, y_pred,\n",
    "                            target_names=labels_for_plot if len(labels_for_plot)==cm.shape[0] else None,\n",
    "                            digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOUzQm-ORjNa"
   },
   "source": [
    "## Part 3: Waste Description Classification\n",
    "\n",
    "In this section, you will build a text classification model to categorize waste based on descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFvG5POxRjNa"
   },
   "source": [
    "### 3.1 Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkl2mkbTRjNa"
   },
   "outputs": [],
   "source": [
    "# ==== 3.1 — Preprocess Text Data ====\n",
    "# What this cell does:\n",
    "# - Loads waste_descriptions.csv from common locations\n",
    "# - Cleans text\n",
    "# - Stratified split (train/test)\n",
    "# - Builds TF-IDF features: word 1–2 grams + char 3–5 grams (fit on TRAIN only)\n",
    "# - Saves artifacts for reuse; on reuse loads them WITHOUT re-fitting label encoders\n",
    "# - Exposes helpers: vectorize_texts(new_texts), idx2label, label2idx\n",
    "\n",
    "# from pathlib import Path\n",
    "# from IPython.display import display\n",
    "# import re, json, os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# from scipy.sparse import hstack\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------- Config / Artifacts ----------\n",
    "ART_DIR = Path(\"artifacts/text_features\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "word_vec_path = ART_DIR / \"word_tfidf.pkl\"\n",
    "char_vec_path = ART_DIR / \"char_tfidf.pkl\"\n",
    "train_pk_path = ART_DIR / \"train_features.pkl\"  # (X_train_feats, y_train)\n",
    "test_pk_path  = ART_DIR / \"test_features.pkl\"   # (X_test_feats,  y_test)\n",
    "labels_path   = ART_DIR / \"label_mapping.json\"  # {\"labels\": [...in fixed order...]}\n",
    "\n",
    "# ---------- Locate dataset ----------\n",
    "CANDIDATES = [\n",
    "    Path(\"/mnt/data/waste_descriptions.csv\"), # if running in hosted env where files are uploaded\n",
    "    Path(\"data/waste_descriptions.csv\"),\n",
    "    Path(\"waste_descriptions.csv\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_descriptions.csv\",\n",
    "]\n",
    "desc_path = next((p for p in CANDIDATES if p.exists()), None)\n",
    "assert desc_path is not None, \"waste_descriptions.csv not found — update CANDIDATES to your actual path.\"\n",
    "\n",
    "df = pd.read_csv(desc_path)\n",
    "\n",
    "# ---------- Column detection ----------\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want or want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col  = pick(df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "label_col = pick(df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "assert text_col and label_col, \"Need at least a text column (description) and a label column (category).\"\n",
    "\n",
    "# ---------- Cleaning ----------\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)   # collapse whitespace\n",
    "    return s.strip()\n",
    "\n",
    "df[text_col]  = df[text_col].astype(str).map(clean_text)\n",
    "df[label_col] = df[label_col].astype(str).str.strip()\n",
    "df = df[df[text_col].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# ---------- Helper to create stable label maps ----------\n",
    "def make_label_maps(labels_list):\n",
    "    \"\"\"Create stable maps using the saved class order.\"\"\"\n",
    "    idx2label = list(labels_list)\n",
    "    label2idx = {lbl: i for i, lbl in enumerate(idx2label)}\n",
    "    return label2idx, idx2label\n",
    "\n",
    "# ---------- Load-or-build pipeline ----------\n",
    "reuse = all(p.exists() for p in [word_vec_path, char_vec_path, train_pk_path, test_pk_path, labels_path])\n",
    "\n",
    "if reuse:\n",
    "    print(\"[Using existing artifacts]\")\n",
    "    word_tfidf = joblib.load(word_vec_path)\n",
    "    char_tfidf = joblib.load(char_vec_path)\n",
    "    (X_train_feats, y_train) = joblib.load(train_pk_path)\n",
    "    (X_test_feats,  y_test)  = joblib.load(test_pk_path)\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = json.load(f)[\"labels\"]  # fixed order from the original fit\n",
    "    label2idx, idx2label = make_label_maps(labels)\n",
    "\n",
    "else:\n",
    "    print(\"[Artifacts not found — building the pipeline now]\")\n",
    "    X = df[text_col].values\n",
    "    y = df[label_col].values\n",
    "\n",
    "    # Fit LabelEncoder ONCE to establish a fixed class order, then freeze it\n",
    "    le = LabelEncoder().fit(y)\n",
    "    y_idx = le.transform(y)\n",
    "    labels = le.classes_.tolist()\n",
    "    label2idx, idx2label = make_label_maps(labels)\n",
    "\n",
    "    # Stratified split (80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_idx, test_size=0.20, random_state=42, stratify=y_idx\n",
    "    )\n",
    "\n",
    "    # Vectorizers\n",
    "    word_tfidf = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_df=0.9\n",
    "    )\n",
    "    char_tfidf = TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3,5),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "\n",
    "    # Fit on TRAIN only; transform Train/Test\n",
    "    Xw_train = word_tfidf.fit_transform(X_train)\n",
    "    Xw_test  = word_tfidf.transform(X_test)\n",
    "    Xc_train = char_tfidf.fit_transform(X_train)\n",
    "    Xc_test  = char_tfidf.transform(X_test)\n",
    "\n",
    "    X_train_feats = hstack([Xw_train, Xc_train]).tocsr()\n",
    "    X_test_feats  = hstack([Xw_test,  Xc_test]).tocsr()\n",
    "\n",
    "    # Save artifacts for future reuse\n",
    "    joblib.dump(word_tfidf, word_vec_path)\n",
    "    joblib.dump(char_tfidf, char_vec_path)\n",
    "    joblib.dump((X_train_feats, y_train), train_pk_path)\n",
    "    joblib.dump((X_test_feats,  y_test),  test_pk_path)\n",
    "    with open(labels_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"labels\": labels}, f, indent=2)\n",
    "\n",
    "# ---------- Report ----------\n",
    "print(\"Feature shapes:\", X_train_feats.shape, X_test_feats.shape)\n",
    "print(\"Classes:\", labels)\n",
    "\n",
    "# Class distribution preview\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index().rename(index=dict(enumerate(labels)))\n",
    "test_counts  = pd.Series(y_test).value_counts().sort_index().rename(index=dict(enumerate(labels)))\n",
    "print(\"\\nTrain class counts:\")\n",
    "display(train_counts)\n",
    "print(\"\\nTest class counts:\")\n",
    "display(test_counts)\n",
    "\n",
    "# ---------- Helpers for later steps ----------\n",
    "def vectorize_texts(texts):\n",
    "    \"\"\"Vectorize a list of raw texts into the same feature space as the training set.\"\"\"\n",
    "    texts = [clean_text(t) for t in texts]\n",
    "    Xw = word_tfidf.transform(texts)\n",
    "    Xc = char_tfidf.transform(texts)\n",
    "    return hstack([Xw, Xc]).tocsr()\n",
    "\n",
    "def decode_indices(y_idx_array):\n",
    "    \"\"\"Convert integer class indices back to class labels using saved order.\"\"\"\n",
    "    return [idx2label[int(i)] for i in y_idx_array]\n",
    "\n",
    "print(\"\\nReady: use `vectorize_texts(['sample text'])` to obtain TF-IDF features for new inputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBO_6HtRRjNa"
   },
   "source": [
    "### 3.2 Implement Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgxh54VcRjNb"
   },
   "outputs": [],
   "source": [
    "# ==== 3.2A — Traditional ML text classifier (LogReg / LinearSVC / ComplementNB) ====\n",
    "# Requirements: 3.1 has produced TF-IDF artifacts in artifacts/text_features/*\n",
    "# What this cell does:\n",
    "# - Loads TF-IDF train/test features and label mapping\n",
    "# - Trains 3 classic models (LogReg, LinearSVC, ComplementNB)\n",
    "# - Selects best by macro F1 on the test set\n",
    "# - Saves the best model and exposes predict() with the SAME preprocessing as 3.1\n",
    "\n",
    "from pathlib import Path\n",
    "import json, joblib, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.sparse import hstack\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "ART_DIR = Path(\"artifacts/text_features\")\n",
    "assert (ART_DIR / \"train_features.pkl\").exists(), \"Run 3.1 first (to build TF-IDF features).\"\n",
    "\n",
    "# --- Load features and labels ---\n",
    "(X_train, y_train) = joblib.load(ART_DIR / \"train_features.pkl\")\n",
    "(X_test,  y_test)  = joblib.load(ART_DIR / \"test_features.pkl\")\n",
    "labels = json.loads((ART_DIR / \"label_mapping.json\").read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "num_classes = len(labels)\n",
    "\n",
    "print(\"Feature dims -> train:\", X_train.shape, \"| test:\", X_test.shape)\n",
    "print(\"Classes:\", labels)\n",
    "\n",
    "# --- Models training & evaluation ---\n",
    "def train_and_eval(X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "\n",
    "    # 1) Logistic Regression (strong baseline for sparse TF-IDF)\n",
    "    logreg = LogisticRegression(\n",
    "        solver=\"saga\", max_iter=3000, n_jobs=-1, class_weight=\"balanced\", C=2.0, random_state=42\n",
    "    ).fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    results[\"logreg\"] = {\n",
    "        \"acc\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"model\": logreg, \"pred\": y_pred\n",
    "    }\n",
    "\n",
    "    # 2) Linear SVM (very strong with TF-IDF)\n",
    "    lsvc = LinearSVC(class_weight=\"balanced\", random_state=42).fit(X_train, y_train)\n",
    "    y_pred = lsvc.predict(X_test)\n",
    "    results[\"linear_svc\"] = {\n",
    "        \"acc\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"model\": lsvc, \"pred\": y_pred\n",
    "    }\n",
    "\n",
    "    # 3) Complement Naive Bayes (fast, robust to imbalance)\n",
    "    cnb = ComplementNB().fit(X_train, y_train)\n",
    "    y_pred = cnb.predict(X_test)\n",
    "    results[\"complement_nb\"] = {\n",
    "        \"acc\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"model\": cnb, \"pred\": y_pred\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "results = train_and_eval(X_train, y_train, X_test, y_test)\n",
    "best_name = max(results, key=lambda k: results[k][\"f1\"])\n",
    "best = results[best_name]\n",
    "print(f\"\\nBest model: {best_name} | acc={best['acc']:.4f} | macro_f1={best['f1']:.4f}\")\n",
    "\n",
    "# --- Labeled report + confusion matrix ---\n",
    "print(\"\\nClassification report (best):\")\n",
    "print(classification_report(y_test, best[\"pred\"], target_names=labels, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, best[\"pred\"], labels=list(range(num_classes)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(f\"Confusion Matrix — {best_name}\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Save the best model ---\n",
    "MODEL_DIR = Path(\"artifacts/text_models\"); MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_path = MODEL_DIR / f\"best_{best_name}.joblib\"\n",
    "joblib.dump(best[\"model\"], model_path)\n",
    "print(\"Saved model to:\", model_path)\n",
    "\n",
    "# --- Load TF-IDF vectorizers for inference ---\n",
    "word_vec = joblib.load(ART_DIR / \"word_tfidf.pkl\")\n",
    "char_vec = joblib.load(ART_DIR / \"char_tfidf.pkl\")\n",
    "\n",
    "# same cleaner as 3.1\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def vectorize_texts(texts):\n",
    "    \"\"\"Vectorize raw texts using the exact TF-IDF feature space from 3.1.\"\"\"\n",
    "    texts = [clean_text(t) for t in texts]\n",
    "    Xw = word_vec.transform(texts)\n",
    "    Xc = char_vec.transform(texts)\n",
    "    return hstack([Xw, Xc]).tocsr()\n",
    "\n",
    "def predict(texts):\n",
    "    \"\"\"Predict class labels (strings) for raw texts.\"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    X = vectorize_texts(texts)\n",
    "    y = model.predict(X)\n",
    "    return [labels[i] for i in y]\n",
    "\n",
    "# Demo\n",
    "print(\"\\nDemo predictions:\")\n",
    "print(predict([\"clear PET bottle with cap\", \"banana peel\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npFcIJCiRjNb"
   },
   "source": [
    "### 3.3 Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KrHYgHWRjNb"
   },
   "outputs": [],
   "source": [
    "# ==== 3.3A — Train & Evaluate (Traditional ML over TF-IDF) ====\n",
    "# Prereq: run 3.1 to create TF-IDF artifacts in artifacts/text_features/*\n",
    "\n",
    "from pathlib import Path\n",
    "import json, joblib, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ART_DIR = Path(\"artifacts/text_features\")\n",
    "assert (ART_DIR / \"train_features.pkl\").exists(), \"Run 3.1 first (build TF-IDF features).\"\n",
    "\n",
    "(X_train, y_train) = joblib.load(ART_DIR / \"train_features.pkl\")\n",
    "(X_test,  y_test)  = joblib.load(ART_DIR / \"test_features.pkl\")\n",
    "labels = json.loads((ART_DIR / \"label_mapping.json\").read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "\n",
    "print(\"Feature dims -> train:\", X_train.shape, \"| test:\", X_test.shape)\n",
    "print(\"Classes:\", labels)\n",
    "\n",
    "# --- Train (Logistic Regression) ---\n",
    "clf = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    max_iter=3000,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    C=2.0,\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "mf1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"\\nTest accuracy: {acc:.4f} | Macro-F1: {mf1:.4f}\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "report_txt = classification_report(y_test, y_pred, target_names=labels, digits=3)\n",
    "print(report_txt)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=list(range(len(labels))))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix — TF-IDF + Logistic Regression\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "per_class_acc = (cm.diagonal() / np.maximum(row_sums.squeeze(), 1)).round(3)\n",
    "per_class_dict = {labels[i]: float(p) for i, p in enumerate(per_class_acc)}\n",
    "print(\"Per-class accuracy:\", per_class_dict)\n",
    "\n",
    "# Top confusion pairs\n",
    "cm_off = cm.copy(); np.fill_diagonal(cm_off, 0)\n",
    "pairs = [((i, j), int(cm_off[i, j])) for i in range(cm_off.shape[0]) for j in range(cm_off.shape[1]) if cm_off[i, j] > 0]\n",
    "pairs = sorted(pairs, key=lambda x: -x[1])[:10]\n",
    "print(\"\\nTop confusion pairs (true -> predicted : count)\")\n",
    "for (i, j), c in pairs:\n",
    "    print(f\"{labels[i]} -> {labels[j]} : {c}\")\n",
    "\n",
    "# --- Save model + artifacts for the report ---\n",
    "MODEL_DIR = Path(\"artifacts/text_models\"); MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_path = MODEL_DIR / \"best_logreg.joblib\"\n",
    "joblib.dump(clf, model_path)\n",
    "print(\"\\nSaved model to:\", model_path)\n",
    "\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "# Save confusion matrix as CSV and metrics as JSON\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"reports/tfidf_logreg_confusion_matrix.csv\")\n",
    "with open(\"reports/tfidf_logreg_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(mf1),\n",
    "        \"per_class_accuracy\": per_class_dict,\n",
    "        \"labels\": labels,\n",
    "        \"top_confusions\": [\n",
    "            {\"true\": labels[i], \"pred\": labels[j], \"count\": c}\n",
    "            for (i, j), c in pairs\n",
    "        ],\n",
    "        \"classification_report_text\": report_txt\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved report to reports/tfidf_logreg_confusion_matrix.csv and reports/tfidf_logreg_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBnmw0EYRjNb"
   },
   "outputs": [],
   "source": [
    "# ==== 3.3B — Train & Evaluate (Transformer: DistilBERT) ====\n",
    "# Trains DistilBERT, monitors metrics, evaluates on held-out test set.\n",
    "# Reuses label order from 3.1 if available to keep indices stable.\n",
    "\n",
    "# %pip -q install transformers datasets accelerate  # uncomment if needed\n",
    "\n",
    "from pathlib import Path\n",
    "import re, json, os\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# --- Load data ---\n",
    "CANDIDATES = [\n",
    "    Path(\"/mnt/data/waste_descriptions.csv\"),\n",
    "    Path(\"data/waste_descriptions.csv\"),\n",
    "    Path(\"waste_descriptions.csv\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_descriptions.csv\",\n",
    "]\n",
    "desc_path = next((p for p in CANDIDATES if p.exists()), None)\n",
    "assert desc_path is not None, \"waste_descriptions.csv not found.\"\n",
    "df = pd.read_csv(desc_path)\n",
    "\n",
    "# --- Column detection ---\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want or want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col = pick(df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "label_col = pick(df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "assert text_col and label_col, \"Need text and label columns.\"\n",
    "\n",
    "# --- Clean ---\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    s = re.sub(r\"\\s+\",\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "df[text_col]  = df[text_col].astype(str).map(clean_text)\n",
    "df[label_col] = df[label_col].astype(str).str.strip()\n",
    "df = df[df[text_col].str.len()>0].reset_index(drop=True)\n",
    "\n",
    "# --- Labels (reuse order from 3.1 if exists) ---\n",
    "ART_TXT = Path(\"artifacts/text_transformer\"); ART_TXT.mkdir(parents=True, exist_ok=True)\n",
    "ART_31  = Path(\"artifacts/text_features\") / \"label_mapping.json\"\n",
    "\n",
    "if ART_31.exists():\n",
    "    labels = json.loads(ART_31.read_text(encoding=\"utf-8\"))[\"labels\"]  # fixed order from 3.1\n",
    "    label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "    # map using saved order; raise if unseen labels appear\n",
    "    unknown = sorted(set(df[label_col]) - set(labels))\n",
    "    assert not unknown, f\"Found unseen labels not present in 3.1 mapping: {unknown}\"\n",
    "    df[\"label_id\"] = df[label_col].map(label2id).astype(int)\n",
    "else:\n",
    "    le = LabelEncoder().fit(df[label_col].values)\n",
    "    labels = le.classes_.tolist()\n",
    "    df[\"label_id\"] = le.transform(df[label_col].values)\n",
    "\n",
    "id2label = {i: l for i, l in enumerate(labels)}\n",
    "label2id = {l: i for i, l in id2label.items()}\n",
    "\n",
    "# --- Split 70/15/15 (seed=42) ---\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, stratify=df[\"label_id\"])\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.50, random_state=42, stratify=temp_df[\"label_id\"])\n",
    "\n",
    "# --- HF dataset ---\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(batch[text_col], truncation=True, max_length=256)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[text_col,\"label_id\"]].rename(columns={\"label_id\":\"labels\"})),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[text_col,\"label_id\"]].rename(columns={\"label_id\":\"labels\"})),\n",
    "    \"test\": Dataset.from_pandas(test_df[[text_col,\"label_id\"]].rename(columns={\"label_id\":\"labels\"})),\n",
    "})\n",
    "ds = ds.map(tokenize, batched=True)\n",
    "\n",
    "# remove only columns that actually exist per split\n",
    "for split in ds.keys():\n",
    "    cols_to_remove = [c for c in [text_col, \"__index_level_0__\"] if c in ds[split].column_names]\n",
    "    if cols_to_remove:\n",
    "        ds[split] = ds[split].remove_columns(cols_to_remove)\n",
    "\n",
    "ds.set_format(\"torch\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# --- Metrics ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels_np = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels_np, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels_np, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1, \"macro_precision\": p, \"macro_recall\": r}\n",
    "\n",
    "# --- Train ---\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(ART_TXT),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_macro_f1\",  # Trainer logs eval_* keys\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=42,  # reproducibility\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tok, compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# --- Evaluate on test ---\n",
    "pred = trainer.predict(ds[\"test\"])\n",
    "metrics = pred.metrics\n",
    "print(\"Test metrics:\", metrics)\n",
    "\n",
    "y_true = test_df[\"label_id\"].to_numpy()\n",
    "y_pred = pred.predictions.argmax(axis=-1)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=labels, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix — DistilBERT\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Per-class accuracy and top confusion pairs\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "per_class_acc = (cm.diagonal() / np.maximum(row_sums.squeeze(), 1)).round(3)\n",
    "print(\"Per-class accuracy:\", {labels[i]: float(p) for i, p in enumerate(per_class_acc)})\n",
    "\n",
    "cm_off = cm.copy(); np.fill_diagonal(cm_off, 0)\n",
    "pairs = [((i,j), int(cm_off[i,j])) for i in range(cm_off.shape[0]) for j in range(cm_off.shape[1]) if cm_off[i,j]>0]\n",
    "pairs = sorted(pairs, key=lambda x: -x[1])[:10]\n",
    "print(\"\\nTop confusion pairs (true -> predicted : count)\")\n",
    "for (i,j), c in pairs:\n",
    "    print(f\"{labels[i]} -> {labels[j]} : {c}\")\n",
    "\n",
    "# (Optional) show a few misclassified examples\n",
    "show_k = 8\n",
    "miss_idx = np.where(y_true != y_pred)[0][:show_k]\n",
    "if len(miss_idx):\n",
    "    print(f\"\\nSample misclassifications (k={len(miss_idx)}):\")\n",
    "    for i in miss_idx:\n",
    "        txt = test_df.iloc[i][text_col]\n",
    "        print(f\"[true={labels[y_true[i]]} | pred={labels[y_pred[i]]}] {txt[:160]}{'...' if len(txt)>160 else ''}\")\n",
    "\n",
    "# Save artifacts\n",
    "trainer.save_model(ART_TXT)\n",
    "tok.save_pretrained(ART_TXT)\n",
    "with open(ART_TXT / \"labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"labels\": labels}, f, indent=2)\n",
    "with open(ART_TXT / \"metrics_3_3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: float(v) for k, v in metrics.items()}, f, indent=2)\n",
    "print(\"\\nSaved model + tokenizer + metrics to:\", ART_TXT.resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUrqcl15RjNb"
   },
   "source": [
    "### 3.4 Create Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0P2ReB_RjNb"
   },
   "outputs": [],
   "source": [
    "# ==== 3.4 — Create Classification Function (auto: transformer -> TF-IDF fallback) ====\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "import numpy as np\n",
    "\n",
    "ART_TXTFEATS = Path(\"artifacts/text_features\")\n",
    "ART_TXTMODEL = Path(\"artifacts/text_models\")\n",
    "ART_TRANS    = Path(\"artifacts/text_transformer\")\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# Cache to avoid reloading on every call\n",
    "_CLS_CACHE = {\n",
    "    \"backend\": None,\n",
    "    \"labels\": None,\n",
    "    \"tfidf\": None,        # (word_tfidf, char_tfidf)\n",
    "    \"clf\": None,          # classic ML model\n",
    "    \"tok\": None,          # HF tokenizer\n",
    "    \"hf_model\": None,     # HF model\n",
    "    \"device\": \"cpu\",      # torch device for HF\n",
    "}\n",
    "\n",
    "def _load_transformer_if_available() -> bool:\n",
    "    \"\"\"Try to load a fine-tuned transformer (tokenizer + model + labels).\"\"\"\n",
    "    if not ART_TRANS.exists():\n",
    "        return False\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        labels_path = ART_TRANS / \"labels.json\"\n",
    "        if not labels_path.exists():\n",
    "            return False\n",
    "        labels = json.loads(labels_path.read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "\n",
    "        tok = AutoTokenizer.from_pretrained(str(ART_TRANS))\n",
    "        hf_model = AutoModelForSequenceClassification.from_pretrained(str(ART_TRANS))\n",
    "\n",
    "        # device & eval mode (safe default: CPU; if CUDA available, you may switch)\n",
    "        device = \"cuda\" if hasattr(torch, \"cuda\") and torch.cuda.is_available() else \"cpu\"\n",
    "        hf_model.to(device)\n",
    "        hf_model.eval()\n",
    "\n",
    "        _CLS_CACHE.update({\n",
    "            \"backend\": \"transformer\",\n",
    "            \"labels\": labels,\n",
    "            \"tok\": tok,\n",
    "            \"hf_model\": hf_model,\n",
    "            \"device\": device,\n",
    "        })\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _load_tfidf_if_available() -> bool:\n",
    "    \"\"\"Load TF-IDF vectorizers + classic model + labels.\"\"\"\n",
    "    try:\n",
    "        import joblib\n",
    "        # Required artifacts\n",
    "        word_vec = ART_TXTFEATS / \"word_tfidf.pkl\"\n",
    "        char_vec = ART_TXTFEATS / \"char_tfidf.pkl\"\n",
    "        labels_path = ART_TXTFEATS / \"label_mapping.json\"\n",
    "        if not (word_vec.exists() and char_vec.exists() and labels_path.exists()):\n",
    "            return False\n",
    "\n",
    "        word_tfidf = joblib.load(word_vec)\n",
    "        char_tfidf = joblib.load(char_vec)\n",
    "        labels = json.loads(labels_path.read_text(encoding=\"utf-8\"))[\"labels\"]\n",
    "\n",
    "        # Prefer 3.3A artifact; otherwise any best_*.joblib from 3.2A\n",
    "        model_path = ART_TXTMODEL / \"best_logreg.joblib\"\n",
    "        if not model_path.exists():\n",
    "            cands = sorted(ART_TXTMODEL.glob(\"best_*.joblib\"))\n",
    "            if not cands:\n",
    "                return False\n",
    "            model_path = cands[0]\n",
    "        clf = joblib.load(model_path)\n",
    "\n",
    "        _CLS_CACHE.update({\n",
    "            \"backend\": \"tfidf\",\n",
    "            \"labels\": labels,\n",
    "            \"tfidf\": (word_tfidf, char_tfidf),\n",
    "            \"clf\": clf\n",
    "        })\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _ensure_backend() -> bool:\n",
    "    \"\"\"Ensure we have a loaded backend in cache (transformer preferred).\"\"\"\n",
    "    if _CLS_CACHE[\"backend\"] is not None:\n",
    "        return True\n",
    "    # Try transformer first\n",
    "    if _load_transformer_if_available():\n",
    "        return True\n",
    "    # Fallback to TF-IDF\n",
    "    if _load_tfidf_if_available():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def classify_waste_description(description: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies a waste description into an appropriate category.\n",
    "\n",
    "    Args:\n",
    "        description (str): Text description of waste item\n",
    "    Returns:\n",
    "        str: Predicted waste category\n",
    "    \"\"\"\n",
    "    assert description is not None and str(description).strip() != \"\", \"Description must be a non-empty string.\"\n",
    "    ok = _ensure_backend()\n",
    "    assert ok, (\n",
    "        \"No text classification backend found. \"\n",
    "        \"Train a model first (run 3.2/3.3) or ensure artifacts exist under \"\n",
    "        \"`artifacts/text_transformer` or `artifacts/text_features` + `artifacts/text_models`.\"\n",
    "    )\n",
    "\n",
    "    text = _clean_text(description)\n",
    "    labels = _CLS_CACHE[\"labels\"]\n",
    "\n",
    "    if _CLS_CACHE[\"backend\"] == \"transformer\":\n",
    "        # HuggingFace inference\n",
    "        import torch\n",
    "        tok = _CLS_CACHE[\"tok\"]; model = _CLS_CACHE[\"hf_model\"]; device = _CLS_CACHE[\"device\"]\n",
    "        enc = tok([text], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            probs = out.logits.softmax(dim=-1)\n",
    "            idx = int(probs.argmax(dim=-1).cpu().numpy()[0])\n",
    "        return labels[idx]\n",
    "\n",
    "    elif _CLS_CACHE[\"backend\"] == \"tfidf\":\n",
    "        # Classic ML over TF-IDF\n",
    "        import scipy.sparse as sp\n",
    "        word_vec, char_vec = _CLS_CACHE[\"tfidf\"]\n",
    "        Xw = word_vec.transform([text])\n",
    "        Xc = char_vec.transform([text])\n",
    "        X = sp.hstack([Xw, Xc]).tocsr()\n",
    "        clf = _CLS_CACHE[\"clf\"]\n",
    "        idx = int(clf.predict(X)[0])\n",
    "        return labels[idx]\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown backend in cache.\")\n",
    "\n",
    "# Optional: probability helper for UI/reporting\n",
    "def classify_waste_description_proba(description: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Returns top-k (label, score) pairs. Uses transformer softmax or approximated scores for TF-IDF models.\n",
    "    \"\"\"\n",
    "    assert description is not None and str(description).strip() != \"\", \"Description must be a non-empty string.\"\n",
    "    ok = _ensure_backend()\n",
    "    assert ok, \"No backend loaded. Train and save models first.\"\n",
    "\n",
    "    text = _clean_text(description)\n",
    "    labels = _CLS_CACHE[\"labels\"]\n",
    "\n",
    "    if _CLS_CACHE[\"backend\"] == \"transformer\":\n",
    "        import torch\n",
    "        tok = _CLS_CACHE[\"tok\"]; model = _CLS_CACHE[\"hf_model\"]; device = _CLS_CACHE[\"device\"]\n",
    "        enc = tok([text], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            probs = out.logits.softmax(dim=-1).cpu().numpy()[0]\n",
    "        order = np.argsort(-probs)[:top_k]\n",
    "        return [(labels[i], float(probs[i])) for i in order]\n",
    "\n",
    "    else:\n",
    "        # TF-IDF backend: try to get calibrated probabilities if available\n",
    "        import scipy.sparse as sp\n",
    "        clf = _CLS_CACHE[\"clf\"]\n",
    "        word_vec, char_vec = _CLS_CACHE[\"tfidf\"]\n",
    "        X = sp.hstack([word_vec.transform([text]), char_vec.transform([text])]).tocsr()\n",
    "\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            proba = clf.predict_proba(X)[0]\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            # Convert margins to pseudo-probabilities via softmax\n",
    "            margins = clf.decision_function(X)\n",
    "            margins = np.atleast_2d(margins)[0]\n",
    "            # softmax\n",
    "            e = np.exp(margins - margins.max())\n",
    "            proba = e / e.sum()\n",
    "        else:\n",
    "            # Last resort: 1 for predicted class, 0 otherwise\n",
    "            idx = int(clf.predict(X)[0])\n",
    "            proba = np.zeros(len(labels), dtype=float)\n",
    "            proba[idx] = 1.0\n",
    "\n",
    "        order = np.argsort(-proba)[:top_k]\n",
    "        return [(labels[i], float(proba[i])) for i in order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kV487HBRjNb"
   },
   "source": [
    "## Part 4: Recycling Instruction Generation with RAG\n",
    "\n",
    "In this section, you will implement a Retrieval-Augmented Generation (RAG) system to generate recycling instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piGWJKgiRjNb"
   },
   "source": [
    "### 4.1 Preprocess Documents for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwZA_B6_RjNb"
   },
   "outputs": [],
   "source": [
    "# ==== 4.1 — Preprocess Documents for Retrieval (Policies + Disposal Instructions) ====\n",
    "# Loads policy docs + disposal instructions, builds chunks, and creates a retrieval index\n",
    "# Preferred: Sentence-Transformers + FAISS; Fallback: TF-IDF + cosine\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# -------- Locate input files --------\n",
    "POL_JSON_CAND = [\n",
    "    Path(\"/mnt/data/waste_policy_documents.json\"),\n",
    "    Path(\"data/waste_policy_documents.json\"),\n",
    "    Path(\"waste_policy_documents.json\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_policy_documents.json\",\n",
    "]\n",
    "POL_CSV_CAND = [\n",
    "    Path(\"/mnt/data/waste_policy_documents.csv\"),\n",
    "    Path(\"data/waste_policy_documents.csv\"),\n",
    "    Path(\"waste_policy_documents.csv\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_policy_documents.csv\",\n",
    "]\n",
    "DESCR_CAND = [\n",
    "    Path(\"/mnt/data/waste_descriptions.csv\"),\n",
    "    Path(\"data/waste_descriptions.csv\"),\n",
    "    Path(\"waste_descriptions.csv\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_descriptions.csv\",\n",
    "]\n",
    "\n",
    "pol_json = next((p for p in POL_JSON_CAND if p.exists()), None)\n",
    "pol_csv  = next((p for p in POL_CSV_CAND if p.exists()), None)\n",
    "desc_csv = next((p for p in DESCR_CAND if p.exists()), None)\n",
    "assert (pol_json or pol_csv), \"Policy docs not found (JSON or CSV).\"\n",
    "assert desc_csv is not None, \"waste_descriptions.csv not found.\"\n",
    "\n",
    "# -------- Utilities --------\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want or want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def to_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    if isinstance(x, str) and x.strip().startswith(\"[\"):\n",
    "        try:\n",
    "            arr = json.loads(x)\n",
    "            if isinstance(arr, list): return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [x] if x is not None else []\n",
    "\n",
    "def chunk_text(text, target_chars=520, overlap=100):\n",
    "    txt = clean_text(text)\n",
    "    sections = re.split(r\"\\n\\s*\\n\", txt)\n",
    "    chunks, buf = [], \"\"\n",
    "    for sec in sections:\n",
    "        if len(buf) + len(sec) + 1 <= target_chars:\n",
    "            buf = (buf + \"\\n\" + sec).strip()\n",
    "        else:\n",
    "            if buf: chunks.append(buf)\n",
    "            if len(sec) > target_chars:\n",
    "                start = 0\n",
    "                while start < len(sec):\n",
    "                    end = min(len(sec), start + target_chars)\n",
    "                    piece = sec[start:end]\n",
    "                    chunks.append(piece.strip())\n",
    "                    start = max(end - overlap, start + 1)\n",
    "            else:\n",
    "                buf = sec\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    chunks = [c for c in chunks if len(c.split()) >= 12]\n",
    "    return chunks\n",
    "\n",
    "# -------- Load policy docs --------\n",
    "if pol_json:\n",
    "    with open(pol_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        pol_data = json.load(f)\n",
    "    pol_df = pd.json_normalize(pol_data)\n",
    "else:\n",
    "    pol_df = pd.read_csv(pol_csv)\n",
    "\n",
    "p_text = pick(pol_df.columns, [\"document_text\",\"text\",\"body\",\"content\"])\n",
    "p_cats = pick(pol_df.columns, [\"categories_covered\",\"category\",\"categories\"])\n",
    "p_juri = pick(pol_df.columns, [\"jurisdiction\",\"city\",\"region\"])\n",
    "p_type = pick(pol_df.columns, [\"policy_type\",\"type\"])\n",
    "p_id   = pick(pol_df.columns, [\"policy_id\",\"id\",\"doc_id\",\"uid\"])\n",
    "assert p_text, \"Need a text/content column in policy docs.\"\n",
    "\n",
    "if p_id is None:\n",
    "    pol_df[\"_pid_auto\"] = np.arange(len(pol_df))\n",
    "    p_id = \"_pid_auto\"\n",
    "\n",
    "rows = []\n",
    "for _, r in pol_df.iterrows():\n",
    "    pid = r[p_id]\n",
    "    juri = r[p_juri] if p_juri else None\n",
    "    ptype = r[p_type] if p_type else None\n",
    "    cats = to_list(r[p_cats]) if p_cats else [None]\n",
    "    text = r[p_text] if pd.notna(r[p_text]) else \"\"\n",
    "    parts = chunk_text(text, target_chars=520, overlap=100)\n",
    "    if not parts:\n",
    "        continue\n",
    "    for cat in (cats if cats else [None]):\n",
    "        for i, ch in enumerate(parts):\n",
    "            rows.append({\n",
    "                \"doc_id\": f\"policy:{pid}:{i}\",\n",
    "                \"source\": \"policy\",\n",
    "                \"policy_id\": pid,\n",
    "                \"policy_type\": ptype,\n",
    "                \"category\": cat,\n",
    "                \"jurisdiction\": juri,\n",
    "                \"text\": ch\n",
    "            })\n",
    "policy_chunks = pd.DataFrame(rows)\n",
    "\n",
    "# -------- Load disposal instructions --------\n",
    "desc_df = pd.read_csv(desc_csv)\n",
    "d_text = pick(desc_df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "d_label= pick(desc_df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "d_disp = pick(desc_df.columns, [\"disposal_instruction\",\"proper_disposal\",\"disposal\",\"instructions\"])\n",
    "assert d_label, \"Need a category/label column in waste_descriptions.csv.\"\n",
    "\n",
    "disp_rows = []\n",
    "if d_disp and d_text:\n",
    "    for _, r in desc_df.dropna(subset=[d_disp]).iterrows():\n",
    "        cat = r[d_label]\n",
    "        instr = clean_text(str(r[d_disp]))\n",
    "        if instr and len(instr.split()) >= 6:\n",
    "            desc_txt = clean_text(str(r[d_text])) if pd.notna(r[d_text]) else \"\"\n",
    "            combined = f\"Disposal instruction:\\n{instr}\\n\\nRelated description: {desc_txt}\" if desc_txt else f\"Disposal instruction:\\n{instr}\"\n",
    "            disp_rows.append({\n",
    "                \"doc_id\": f\"disposal:{len(disp_rows)}\",\n",
    "                \"source\": \"disposal\",\n",
    "                \"policy_id\": None,\n",
    "                \"policy_type\": None,\n",
    "                \"category\": cat,\n",
    "                \"jurisdiction\": None,\n",
    "                \"text\": combined\n",
    "            })\n",
    "disposal_docs = pd.DataFrame(disp_rows) if disp_rows else pd.DataFrame(columns=[\"doc_id\",\"source\",\"policy_id\",\"policy_type\",\"category\",\"jurisdiction\",\"text\"])\n",
    "\n",
    "# -------- Combine corpora --------\n",
    "docs = pd.concat([policy_chunks, disposal_docs], ignore_index=True).reset_index(drop=True)\n",
    "docs[\"text\"] = docs[\"text\"].fillna(\"\").astype(str).map(clean_text)\n",
    "docs = docs[docs[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(\"Corpus summary:\")\n",
    "display(pd.DataFrame({\"source_counts\": docs[\"source\"].value_counts()}))\n",
    "print(\"Total chunks:\", len(docs))\n",
    "# display(docs.head(8))  # optional: verbose preview\n",
    "\n",
    "# -------- Build embeddings + index --------\n",
    "ART_DIR = Path(\"artifacts/rag_v2\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "use_dense = False\n",
    "faiss_available = False\n",
    "dense_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    use_dense = True\n",
    "    faiss_available = True\n",
    "except Exception as e:\n",
    "    print(\"[Info] Using TF-IDF fallback (Sentence-Transformers/FAISS not available):\", e)\n",
    "\n",
    "if use_dense and faiss_available:\n",
    "    model = SentenceTransformer(dense_model_name)\n",
    "    texts = docs[\"text\"].tolist()\n",
    "    emb = model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    dim = emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(emb.astype(np.float32))\n",
    "    faiss.write_index(index, str(ART_DIR / \"faiss.index\"))\n",
    "    # np.save(ART_DIR / \"embeddings.npy\", emb)  # removed: not used later\n",
    "    docs.to_csv(ART_DIR / \"meta.csv\", index=False)\n",
    "    joblib.dump({\"model_name\": dense_model_name}, ART_DIR / \"dense_info.pkl\")\n",
    "    print(\"Saved dense FAISS index to:\", ART_DIR.resolve())\n",
    "else:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\", ngram_range=(1,2), min_df=1, max_df=1.0)\n",
    "    X = vec.fit_transform(docs[\"text\"])\n",
    "    docs.to_csv(ART_DIR / \"meta.csv\", index=False)\n",
    "    joblib.dump({\"vectorizer\": vec, \"matrix\": X, \"meta_path\": str(ART_DIR / \"meta.csv\")}, ART_DIR / \"tfidf_index.pkl\")\n",
    "    print(\"Saved TF-IDF index to:\", ART_DIR.resolve())\n",
    "\n",
    "# -------- Retrieval helper --------\n",
    "def search_documents(query, top_k=5, category=None, jurisdiction=None, sources=None):\n",
    "    \"\"\"\n",
    "    Search the retrieval index and return top-k results with optional filters.\n",
    "    Returns a DataFrame: score, doc_id, source, category, jurisdiction, text\n",
    "    \"\"\"\n",
    "    if (ART_DIR / \"faiss.index\").exists():\n",
    "        import faiss\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        meta = pd.read_csv(ART_DIR / \"meta.csv\")\n",
    "        model_name = joblib.load(ART_DIR / \"dense_info.pkl\")[\"model_name\"]\n",
    "        model = SentenceTransformer(model_name)\n",
    "        index = faiss.read_index(str(ART_DIR / \"faiss.index\"))\n",
    "\n",
    "        qv = model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "        D, I = index.search(qv, top_k * 10)\n",
    "        D, I = D[0], I[0]\n",
    "        results = []\n",
    "        for i, score in zip(I, D):\n",
    "            row = meta.iloc[int(i)]\n",
    "            if category and str(row[\"category\"]) != str(category): \n",
    "                continue\n",
    "            if jurisdiction and str(row[\"jurisdiction\"]) != str(jurisdiction): \n",
    "                continue\n",
    "            if sources and row[\"source\"] not in set(sources):\n",
    "                continue\n",
    "            results.append({\n",
    "                \"score\": float(score),\n",
    "                \"doc_id\": row[\"doc_id\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"jurisdiction\": row[\"jurisdiction\"],\n",
    "                \"text\": (row[\"text\"][:260] + \"...\") if len(row[\"text\"])>260 else row[\"text\"]\n",
    "            })\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    else:\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        bundle = joblib.load(ART_DIR / \"tfidf_index.pkl\")\n",
    "        vec = bundle[\"vectorizer\"]\n",
    "        X = bundle[\"matrix\"]\n",
    "        meta = pd.read_csv(bundle[\"meta_path\"])\n",
    "\n",
    "        qv = vec.transform([query])\n",
    "        sims = cosine_similarity(qv, X).ravel()\n",
    "        order = np.argsort(-sims)\n",
    "\n",
    "        results = []\n",
    "        for i in order:\n",
    "            row = meta.iloc[int(i)]\n",
    "            if category and str(row[\"category\"]) != str(category): \n",
    "                continue\n",
    "            if jurisdiction and str(row[\"jurisdiction\"]) != str(jurisdiction): \n",
    "                continue\n",
    "            if sources and row[\"source\"] not in set(sources):\n",
    "                continue\n",
    "            results.append({\n",
    "                \"score\": float(sims[i]),\n",
    "                \"doc_id\": row[\"doc_id\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"jurisdiction\": row[\"jurisdiction\"],\n",
    "                \"text\": (row[\"text\"][:260] + \"...\") if len(row[\"text\"])>260 else row[\"text\"]\n",
    "            })\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nReady. Try:\")\n",
    "print(\"search_documents('rinse plastic bottle before recycling', top_k=5, category='plastic', jurisdiction='Metro City')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgzotuLDRjNb"
   },
   "source": [
    "### 4.2 Implement RAG-based System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3Db6G58RjNb"
   },
   "outputs": [],
   "source": [
    "# ==== 4.2 — RAG-based System (Retriever + Generator) ====\n",
    "# Loads the retrieval index (FAISS+dense or TF-IDF sparse) and a seq2seq generator (FLAN-T5/T5)\n",
    "# Implements: rag_generate_instructions(description=None, image_label=None, city=\"Metro City\", top_k=5, ...)\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ART_RAG = Path(\"artifacts/rag_v2\")  # built in 4.1\n",
    "assert ART_RAG.exists(), \"Run 4.1 first to build retrieval artifacts.\"\n",
    "\n",
    "# ---------------- Utilities ----------------\n",
    "def _clean(s: str) -> str:\n",
    "    s = str(s).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _infer_category(description, image_label):\n",
    "    # Prefer trained text classifier (3.4) if available\n",
    "    if \"classify_waste_description\" in globals() and description and str(description).strip():\n",
    "        try:\n",
    "            return classify_waste_description(description)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return image_label\n",
    "\n",
    "# ---------------- Retriever (cached) ----------------\n",
    "_RETR_CACHE = {\"kind\": None, \"search\": None}\n",
    "\n",
    "def _load_retriever():\n",
    "    if _RETR_CACHE[\"search\"] is not None:\n",
    "        return _RETR_CACHE\n",
    "\n",
    "    # Dense (FAISS + Sentence-Transformers)\n",
    "    if (ART_RAG / \"faiss.index\").exists():\n",
    "        import joblib, faiss\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        dense_info = joblib.load(ART_RAG / \"dense_info.pkl\")\n",
    "        model_name = dense_info.get(\"model_name\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        st_model = SentenceTransformer(model_name)\n",
    "        faiss_index = faiss.read_index(str(ART_RAG / \"faiss.index\"))\n",
    "        meta = pd.read_csv(ART_RAG / \"meta.csv\")\n",
    "\n",
    "        def dense_search(query, top_k=5, category=None, jurisdiction=None, sources=None):\n",
    "            qv = st_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "            D, I = faiss_index.search(qv, top_k * 10)  # oversample then filter\n",
    "            D, I = D[0], I[0]\n",
    "            res = []\n",
    "            for i, score in zip(I, D):\n",
    "                row = meta.iloc[int(i)]\n",
    "                if category and str(row[\"category\"]) != str(category): \n",
    "                    continue\n",
    "                if jurisdiction and str(row[\"jurisdiction\"]) != str(jurisdiction): \n",
    "                    continue\n",
    "                if sources and row[\"source\"] not in set(sources): \n",
    "                    continue\n",
    "                res.append({\n",
    "                    \"rank\": len(res)+1,\n",
    "                    \"score\": float(score),\n",
    "                    \"doc_id\": row[\"doc_id\"],\n",
    "                    \"source\": row[\"source\"],\n",
    "                    \"category\": row[\"category\"],\n",
    "                    \"jurisdiction\": row[\"jurisdiction\"],\n",
    "                    \"text\": row[\"text\"]\n",
    "                })\n",
    "                if len(res) >= top_k:\n",
    "                    break\n",
    "            return res\n",
    "\n",
    "        _RETR_CACHE.update({\"kind\": \"dense\", \"search\": dense_search})\n",
    "        return _RETR_CACHE\n",
    "\n",
    "    # Sparse (TF-IDF + cosine)\n",
    "    import joblib\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    bundle = joblib.load(ART_RAG / \"tfidf_index.pkl\")\n",
    "    vec, X, meta_path = bundle[\"vectorizer\"], bundle[\"matrix\"], bundle[\"meta_path\"]\n",
    "    meta = pd.read_csv(meta_path)\n",
    "\n",
    "    def sparse_search(query, top_k=5, category=None, jurisdiction=None, sources=None):\n",
    "        sims = cosine_similarity(vec.transform([query]), X).ravel()\n",
    "        order = np.argsort(-sims)\n",
    "        res = []\n",
    "        for idx in order:\n",
    "            row = meta.iloc[int(idx)]\n",
    "            if category and str(row[\"category\"]) != str(category): \n",
    "                continue\n",
    "            if jurisdiction and str(row[\"jurisdiction\"]) != str(jurisdiction): \n",
    "                continue\n",
    "            if sources and row[\"source\"] not in set(sources): \n",
    "                continue\n",
    "            res.append({\n",
    "                \"rank\": len(res)+1,\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"doc_id\": row[\"doc_id\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"jurisdiction\": row[\"jurisdiction\"],\n",
    "                \"text\": row[\"text\"]\n",
    "            })\n",
    "            if len(res) >= top_k:\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    _RETR_CACHE.update({\"kind\": \"sparse\", \"search\": sparse_search})\n",
    "    return _RETR_CACHE\n",
    "\n",
    "RETRIEVER = _load_retriever()\n",
    "print(f\"[RAG] Retriever ready: {RETRIEVER['kind']}\")\n",
    "\n",
    "# ---------------- Generator (cached) ----------------\n",
    "_GEN_CACHE = {\"name\": None, \"tok\": None, \"mdl\": None, \"device\": \"cpu\"}\n",
    "GEN_MODEL_NAME_CANDIDATES = [\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-small\",\n",
    "    \"t5-small\",\n",
    "]\n",
    "\n",
    "def _load_generator():\n",
    "    if _GEN_CACHE[\"tok\"] is not None:\n",
    "        return _GEN_CACHE\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    import torch\n",
    "    for name in GEN_MODEL_NAME_CANDIDATES:\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(name)\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            mdl.to(device)\n",
    "            mdl.eval()\n",
    "            _GEN_CACHE.update({\"name\": name, \"tok\": tok, \"mdl\": mdl, \"device\": device})\n",
    "            return _GEN_CACHE\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(\"No suitable seq2seq model found. Install weights for FLAN-T5 or T5.\")\n",
    "\n",
    "GEN = _load_generator()\n",
    "print(f\"[RAG] Generator ready: {GEN['name']}\")\n",
    "\n",
    "# ---------------- Prompt building ----------------\n",
    "def _build_prompt(passages, category, city, description):\n",
    "    header = (\n",
    "        f\"You are EcoSort, a recycling assistant for {city}. \"\n",
    "        f\"Use ONLY the policy excerpts to produce clear recycling instructions. \"\n",
    "        f\"Waste category: {category if category else 'Unknown'}.\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"• Prefer guidance specific to the given city/jurisdiction.\\n\"\n",
    "        \"• If rules conflict, prefer the most explicit excerpt.\\n\"\n",
    "        \"• If curbside recycling is not allowed, clearly state the correct disposal method.\\n\"\n",
    "        \"• Keep output concise: 5–8 bullet points maximum.\\n\"\n",
    "        \"• Cite passages using [#N] at the end of each bullet.\\n\"\n",
    "        \"• Do not invent facts not present in excerpts.\\n\\n\"\n",
    "    )\n",
    "    if description:\n",
    "        header += f\"Resident description: \\\"{_clean(description)}\\\"\\n\\n\"\n",
    "\n",
    "    ctx_lines = []\n",
    "    for i, p in enumerate(passages, start=1):\n",
    "        meta = f\"(source={p['source']}; doc_id={p['doc_id']}; cat={p['category']}; jur={p['jurisdiction']})\"\n",
    "        txt = _clean(p[\"text\"])\n",
    "        if len(txt) > 900:\n",
    "            txt = txt[:900] + \" ...\"\n",
    "        ctx_lines.append(f\"[#{i}] {meta}\\n{txt}\")\n",
    "    ctx = \"\\n\\n\".join(ctx_lines)\n",
    "\n",
    "    task = (\n",
    "        \"Write recycling instructions with the following sections:\\n\"\n",
    "        \"1) Steps (numbered)\\n\"\n",
    "        \"2) Do\\n\"\n",
    "        \"3) Don't\\n\"\n",
    "        \"4) Bin/Facility\\n\"\n",
    "        \"5) Notes\\n\"\n",
    "        \"Each line should end with a citation like [#N] referencing the excerpt used.\\n\"\n",
    "        \"If no relevant excerpts exist, say 'No city policy found' and provide general best-practice advice without citations.\"\n",
    "    )\n",
    "    return header + \"Policy excerpts:\\n\" + ctx + \"\\n\\n\" + task\n",
    "\n",
    "def _generate(prompt, max_new_tokens=220, temperature=0.2):\n",
    "    from transformers import set_seed\n",
    "    import torch\n",
    "    set_seed(42)\n",
    "    tok, mdl, device = GEN[\"tok\"], GEN[\"mdl\"], GEN[\"device\"]\n",
    "    enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = mdl.generate(\n",
    "            **enc,\n",
    "            do_sample=(temperature > 0),\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4 if temperature == 0 else 1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# ---------------- Main entrypoint ----------------\n",
    "def rag_generate_instructions(description=None, image_label=None, city=\"Metro City\", top_k=5,\n",
    "                              sources=None, max_new_tokens=220, temperature=0.2):\n",
    "    \"\"\"\n",
    "    Retrieve policy/disposal excerpts and generate grounded instructions.\n",
    "\n",
    "    Returns:\n",
    "        dict with: category, city, retrieved (ranked passages), instructions (text)\n",
    "    \"\"\"\n",
    "    category = _infer_category(description, image_label)\n",
    "\n",
    "    # Compose retrieval query blending description + category + city\n",
    "    q_parts = []\n",
    "    if description: q_parts.append(_clean(description))\n",
    "    if category:    q_parts.append(f\"Category: {category}\")\n",
    "    if city:        q_parts.append(f\"Jurisdiction: {city}\")\n",
    "    query = \" | \".join(q_parts) if q_parts else \"recycling policy\"\n",
    "\n",
    "    # Retrieve with narrowing filters, then relax if empty\n",
    "    recs = RETRIEVER[\"search\"](query, top_k=top_k, category=category, jurisdiction=city, sources=sources)\n",
    "    if not recs:\n",
    "        recs = RETRIEVER[\"search\"](query, top_k=top_k, category=category, jurisdiction=None, sources=sources)\n",
    "        if not recs:\n",
    "            recs = RETRIEVER[\"search\"](query, top_k=top_k, category=None, jurisdiction=None, sources=sources)\n",
    "\n",
    "    prompt = _build_prompt(recs, category, city, description)\n",
    "    instructions = _generate(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"city\": city,\n",
    "        \"retrieved\": [\n",
    "            {\"rank\": r[\"rank\"], \"doc_id\": r[\"doc_id\"], \"source\": r[\"source\"],\n",
    "             \"category\": r[\"category\"], \"jurisdiction\": r[\"jurisdiction\"],\n",
    "             \"text\": (r[\"text\"][:220] + \"...\") if len(r[\"text\"])>220 else r[\"text\"]}\n",
    "            for r in recs\n",
    "        ],\n",
    "        \"instructions\": instructions\n",
    "    }\n",
    "\n",
    "# -------- Demo (uncomment if you want to see it run) --------\n",
    "demo = rag_generate_instructions(\n",
    "    description=\"Clear PET plastic bottle with cap\",\n",
    "    image_label=None,\n",
    "    city=\"Metro City\",\n",
    "    top_k=5,\n",
    "    temperature=0.0\n",
    ")\n",
    "print(demo[\"instructions\"])\n",
    "pd.DataFrame(demo[\"retrieved\"]).head()\n",
    "\n",
    "res = rag_generate_instructions(\n",
    "    description=\"Brown glass beer bottle with label\",\n",
    "    city=\"Metro City\",\n",
    "    top_k=5,\n",
    "    temperature=0.0\n",
    ")\n",
    "print(res[\"instructions\"])\n",
    "pd.DataFrame(res[\"retrieved\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv-tCvY8RjNb"
   },
   "source": [
    "### 4.3 Adjust and Evaluate the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qX5AkI7LRjNh"
   },
   "outputs": [],
   "source": [
    "# ==== 4.3A — RAG parameter sweep (decoding + retrieval) ====\n",
    "# Builds a small balanced eval set, sweeps RAG params, and stores generations to artifacts/rag_eval/<run_id>.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import json, time, itertools, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "assert \"rag_generate_instructions\" in globals(), \"Run 4.2 first to define rag_generate_instructions().\"\n",
    "\n",
    "random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- Locate and read dataset ----------\n",
    "CANDIDATES = [\n",
    "    Path(\"/mnt/data/waste_descriptions.csv\"),\n",
    "    Path(\"data/waste_descriptions.csv\"),\n",
    "    Path(\"waste_descriptions.csv\"),\n",
    "    Path.home() / \"Downloads\" / \"waste_descriptions.csv\",\n",
    "]\n",
    "desc_path = next((p for p in CANDIDATES if p.exists()), None)\n",
    "assert desc_path is not None, \"waste_descriptions.csv not found.\"\n",
    "\n",
    "df = pd.read_csv(desc_path)\n",
    "\n",
    "def pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want or want in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "text_col = pick(df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "label_col = pick(df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "disp_col  = pick(df.columns, [\"disposal_instruction\",\"proper_disposal\",\"disposal\",\"instructions\"])\n",
    "assert text_col and label_col, \"Need text and label columns.\"\n",
    "\n",
    "# ---------- Build a small balanced eval set ----------\n",
    "N_PER_CAT = 2  # increase to 3–5 for a deeper run\n",
    "rows = []\n",
    "for cat, sub in df.groupby(label_col):\n",
    "    sub = sub.dropna(subset=[text_col])\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    sample = sub.sample(min(N_PER_CAT, len(sub)), random_state=42)\n",
    "    for _, r in sample.iterrows():\n",
    "        rows.append({\n",
    "            \"category\": str(cat),\n",
    "            \"description\": str(r[text_col]),\n",
    "            \"reference_disposal\": (str(r[disp_col]) if disp_col and pd.notna(r[disp_col]) else None)\n",
    "        })\n",
    "eval_df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "print(f\"Eval set size: {len(eval_df)} across {eval_df['category'].nunique()} categories\")\n",
    "\n",
    "# ---------- Parameter grid ----------\n",
    "TEMPS   = [0.0, 0.2]           # 0.0 = deterministic beams, >0 = sampling\n",
    "TOPK    = [4, 6]\n",
    "SOURCES = [None, [\"policy\"]]   # None = policy + disposal; [\"policy\"] restricts to policy excerpts only\n",
    "MAX_NEW = [180]                # generation cap\n",
    "CITY    = \"Metro City\"\n",
    "\n",
    "combos = list(itertools.product(TEMPS, TOPK, SOURCES, MAX_NEW))\n",
    "print(\"Trials:\", len(combos), \":\", combos)\n",
    "\n",
    "# ---------- Run sweep ----------\n",
    "run_id = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = Path(\"artifacts/rag_eval\") / run_id\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "records = []\n",
    "for (temp, topk, sources, max_new) in combos:\n",
    "    cfg_name = f\"temp={temp}|topk={topk}|src={'all' if sources is None else ','.join(sources)}|max={max_new}\"\n",
    "    for i, row in eval_df.iterrows():\n",
    "        desc = row[\"description\"]\n",
    "        cat  = row[\"category\"]\n",
    "        ref  = row[\"reference_disposal\"]\n",
    "\n",
    "        try:\n",
    "            out = rag_generate_instructions(\n",
    "                description=desc,\n",
    "                image_label=None,   # optionally pass your CNN label to force category\n",
    "                city=CITY,\n",
    "                top_k=topk,\n",
    "                sources=sources,\n",
    "                max_new_tokens=max_new,\n",
    "                temperature=temp\n",
    "            )\n",
    "\n",
    "            rec_meta = [{\"rank\": r[\"rank\"], \"doc_id\": r[\"doc_id\"], \"source\": r[\"source\"],\n",
    "                         \"category\": r[\"category\"], \"jurisdiction\": r[\"jurisdiction\"]}\n",
    "                        for r in out[\"retrieved\"]]\n",
    "\n",
    "            records.append({\n",
    "                \"cfg\": cfg_name,\n",
    "                \"temperature\": temp,\n",
    "                \"top_k\": topk,\n",
    "                \"sources\": None if sources is None else \"|\".join(sources),\n",
    "                \"max_new_tokens\": max_new,\n",
    "                \"idx\": i,\n",
    "                \"category\": cat,\n",
    "                \"description\": desc,\n",
    "                \"reference_disposal\": ref,\n",
    "                \"pred_category_used\": out[\"category\"],\n",
    "                \"retrieved_json\": json.dumps(rec_meta, ensure_ascii=False),\n",
    "                \"retrieved_n\": len(out[\"retrieved\"]),\n",
    "                \"instructions\": out[\"instructions\"],\n",
    "                \"error\": None\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # log the error but keep the sweep running\n",
    "            records.append({\n",
    "                \"cfg\": cfg_name,\n",
    "                \"temperature\": temp,\n",
    "                \"top_k\": topk,\n",
    "                \"sources\": None if sources is None else \"|\".join(sources),\n",
    "                \"max_new_tokens\": max_new,\n",
    "                \"idx\": i,\n",
    "                \"category\": cat,\n",
    "                \"description\": desc,\n",
    "                \"reference_disposal\": ref,\n",
    "                \"pred_category_used\": None,\n",
    "                \"retrieved_json\": \"[]\",\n",
    "                \"retrieved_n\": 0,\n",
    "                \"instructions\": \"\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "EVAL_RAW = pd.DataFrame(records)\n",
    "csv_path = OUT_DIR / \"generations.csv\"\n",
    "EVAL_RAW.to_csv(csv_path, index=False)\n",
    "print(\"Saved generations to:\", csv_path.resolve())\n",
    "\n",
    "# Keep path in a variable for downstream analysis\n",
    "EVAL_RAW_PATH = str(csv_path)\n",
    "print(\"EVAL_RAW_PATH =\", EVAL_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3RfCq24RjNh"
   },
   "outputs": [],
   "source": [
    "# ==== 4.3B — Evaluate RAG outputs (metrics + qualitative examples) ====\n",
    "# Loads generations CSV, computes tok-overlap F1, citation validity, retrieval mix, ranks configs, and provides inspectors.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Load ----------\n",
    "if \"EVAL_RAW\" in globals():\n",
    "    df = EVAL_RAW.copy()\n",
    "else:\n",
    "    EVAL_RAW_PATH = globals().get(\"EVAL_RAW_PATH\", None)\n",
    "    assert EVAL_RAW_PATH is not None, \"Set EVAL_RAW_PATH to artifacts/rag_eval/<run_id>/generations.csv\"\n",
    "    df = pd.read_csv(EVAL_RAW_PATH)\n",
    "\n",
    "# Ensure required columns exist (guard against partial runs)\n",
    "for col in [\"instructions\", \"retrieved_n\", \"retrieved_json\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\" if col == \"retrieved_json\" else 0\n",
    "\n",
    "# ---------- Metrics helpers ----------\n",
    "_ws = re.compile(r\"[^\\w]+\", re.UNICODE)\n",
    "\n",
    "def _tokens(s):\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)): \n",
    "        return []\n",
    "    s = str(s).lower()\n",
    "    toks = [t for t in _ws.split(s) if len(t) >= 2]\n",
    "    return toks\n",
    "\n",
    "def token_f1(pred, ref):\n",
    "    \"\"\"Unigram overlap F1 between generated instructions and reference disposal text (if any).\"\"\"\n",
    "    P = set(_tokens(pred))\n",
    "    R = set(_tokens(ref))\n",
    "    if not P or not R:\n",
    "        return np.nan\n",
    "    inter = len(P & R)\n",
    "    prec = inter / max(len(P), 1)\n",
    "    rec  = inter / max(len(R), 1)\n",
    "    if (prec + rec) == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "_BULLET_RE = re.compile(r\"^(\\d+[\\.\\)]|-|\\*|•)\\s*\")  # supports \"1.\" / \"1)\" / \"-\" / \"*\" / \"•\"\n",
    "_CITE_RE   = re.compile(r\"\\[#(\\d+)\\]\\s*$\")\n",
    "\n",
    "def citation_validity_rate(text, retrieved_n):\n",
    "    \"\"\"\n",
    "    Share of bullet/numbered lines that end with a valid [#N], where 1 <= N <= retrieved_n.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or int(retrieved_n) <= 0:\n",
    "        return 0.0\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return 0.0\n",
    "    good, total = 0, 0\n",
    "    for ln in lines:\n",
    "        if _BULLET_RE.match(ln):\n",
    "            total += 1\n",
    "            m = _CITE_RE.search(ln)\n",
    "            if m:\n",
    "                k = int(m.group(1))\n",
    "                if 1 <= k <= int(retrieved_n):\n",
    "                    good += 1\n",
    "    return (good / total) if total else 0.0\n",
    "\n",
    "def retrieval_policy_share(retrieved_json):\n",
    "    \"\"\"Fraction of retrieved items that come from 'policy' source.\"\"\"\n",
    "    try:\n",
    "        items = json.loads(retrieved_json)\n",
    "        if not items:\n",
    "            return np.nan\n",
    "        pol = sum(1 for it in items if str(it.get(\"source\")) == \"policy\")\n",
    "        return pol / len(items)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ---------- Compute per-row metrics ----------\n",
    "if \"reference_disposal\" in df.columns:\n",
    "    df[\"tokF1\"] = df.apply(lambda r: token_f1(r[\"instructions\"], r[\"reference_disposal\"]), axis=1)\n",
    "else:\n",
    "    df[\"tokF1\"] = np.nan\n",
    "\n",
    "df[\"cite_rate\"]   = df.apply(lambda r: citation_validity_rate(r[\"instructions\"], r[\"retrieved_n\"]), axis=1)\n",
    "df[\"policy_share\"] = df[\"retrieved_json\"].map(retrieval_policy_share)\n",
    "df[\"len_words\"]    = df[\"instructions\"].fillna(\"\").astype(str).str.split().apply(len)\n",
    "\n",
    "# ---------- Aggregate by config ----------\n",
    "agg = df.groupby(\"cfg\", dropna=False).agg(\n",
    "    n=(\"idx\", \"count\"),\n",
    "    tokF1_mean=(\"tokF1\", \"mean\"),\n",
    "    cite_rate_mean=(\"cite_rate\", \"mean\"),\n",
    "    policy_share_mean=(\"policy_share\", \"mean\"),\n",
    "    len_words_mean=(\"len_words\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Ranking logic:\n",
    "if agg[\"tokF1_mean\"].notna().any():\n",
    "    agg = agg.sort_values(by=[\"tokF1_mean\", \"cite_rate_mean\"], ascending=False)\n",
    "else:\n",
    "    agg = agg.sort_values(by=[\"cite_rate_mean\", \"policy_share_mean\"], ascending=False)\n",
    "\n",
    "print(\"Configs ranked:\")\n",
    "display(agg.head(10))\n",
    "\n",
    "# ---------- Per-category breakdown (top 5 shown) ----------\n",
    "cat_agg = df.groupby([\"cfg\", \"category\"], dropna=False).agg(\n",
    "    n=(\"idx\", \"count\"),\n",
    "    tokF1_mean=(\"tokF1\", \"mean\"),\n",
    "    cite_rate_mean=(\"cite_rate\", \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "print(\"Per-category breakdown (sample):\")\n",
    "# Option A (as before, may warn in future pandas):\n",
    "display(cat_agg.groupby(\"category\").apply(lambda g: g.nlargest(1, columns=[\"tokF1_mean\", \"cite_rate_mean\"])).reset_index(drop=True).head(20))\n",
    "# Option B (no-apply): pick best row per category by sort then drop_duplicates\n",
    "# best_per_cat = cat_agg.sort_values(by=[\"category\", \"tokF1_mean\", \"cite_rate_mean\"], ascending=[True, False, False]).drop_duplicates(\"category\", keep=\"first\")\n",
    "# display(best_per_cat.head(20))\n",
    "\n",
    "# ---------- Qualitative inspection helpers ----------\n",
    "def show_example(row_idx):\n",
    "    \"\"\"Print a single example with its config, description, retrieved heads, and instructions.\"\"\"\n",
    "    r = df.iloc[int(row_idx)]\n",
    "    print(\"CFG:\", r.get(\"cfg\"))\n",
    "    print(\"CATEGORY:\", r.get(\"category\"))\n",
    "    print(\"DESCRIPTION:\", r.get(\"description\"))\n",
    "    print(\"TOK-F1 (if ref):\", r.get(\"tokF1\"), \"| CITE_RATE:\", r.get(\"cite_rate\"), \"| POLICY_SHARE:\", r.get(\"policy_share\"))\n",
    "    print(\"\\nRETRIEVED (top 2):\")\n",
    "    try:\n",
    "        items = json.loads(r.get(\"retrieved_json\", \"[]\"))\n",
    "        for it in items[:2]:\n",
    "            print(f\"  - rank={it.get('rank')} | source={it.get('source')} | doc={it.get('doc_id')} | cat={it.get('category')} | jur={it.get('jurisdiction')}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"\\nINSTRUCTIONS:\\n\", r.get(\"instructions\"))\n",
    "\n",
    "def find_worst(k=5, by=\"tokF1\"):\n",
    "    \"\"\"List k worst examples by a metric ('tokF1' or 'cite_rate').\"\"\"\n",
    "    sub = df.copy()\n",
    "    if by == \"tokF1\":\n",
    "        sub = sub[sub[\"tokF1\"].notna()]\n",
    "        return sub.nsmallest(k, columns=[\"tokF1\"])[[\"cfg\",\"category\",\"idx\",\"tokF1\",\"cite_rate\",\"description\"]]\n",
    "    elif by == \"cite_rate\":\n",
    "        return sub.nsmallest(k, columns=[\"cite_rate\"])[[\"cfg\",\"category\",\"idx\",\"tokF1\",\"cite_rate\",\"description\"]]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown metric.\")\n",
    "\n",
    "print(\"\\nHow to use:\")\n",
    "print(\"- Inspect best configs above; re-run 4.3A with tweaked grids if needed.\")\n",
    "print(\"- show_example(0)  # replace 0 with a row index from df\")\n",
    "print(\"- find_worst(k=5, by='tokF1')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfep5M1BRjNi"
   },
   "source": [
    "### 4.4 Create Instruction Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqy4abs1RjNi"
   },
   "outputs": [],
   "source": [
    "# ==== 4.4 — Instruction generation function (RAG-first, robust fallbacks) ====\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd\n",
    "\n",
    "# Defaults\n",
    "_CITY_DEFAULT   = \"Metro City\"\n",
    "_TOP_K_DEFAULT  = 5\n",
    "_TEMP_DEFAULT   = 0.0\n",
    "_ART_RAG_DIR    = Path(\"artifacts/rag_v2\")  # created in 4.1\n",
    "\n",
    "# Caches\n",
    "_GEN_CACHE = {\"tok\": None, \"mdl\": None, \"name\": None}\n",
    "_RET_CACHE = {\"kind\": None, \"search\": None, \"meta\": None}\n",
    "\n",
    "def _clean(s: str) -> str:\n",
    "    s = str(s).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _ensure_retriever() -> bool:\n",
    "    \"\"\"Load the retriever built in 4.1 (dense FAISS preferred, TF-IDF fallback).\"\"\"\n",
    "    if _RET_CACHE[\"search\"] is not None:\n",
    "        return True\n",
    "    if not _ART_RAG_DIR.exists():\n",
    "        return False\n",
    "\n",
    "    faiss_path = _ART_RAG_DIR / \"faiss.index\"\n",
    "    # Dense (Sentence-Transformers + FAISS)\n",
    "    if faiss_path.exists():\n",
    "        try:\n",
    "            import faiss\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import joblib\n",
    "\n",
    "            meta = pd.read_csv(_ART_RAG_DIR / \"meta.csv\")\n",
    "            dense_info = joblib.load(_ART_RAG_DIR / \"dense_info.pkl\")\n",
    "            model_name = dense_info.get(\"model_name\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            st_model = SentenceTransformer(model_name)\n",
    "            index = faiss.read_index(str(faiss_path))\n",
    "\n",
    "            def _dense_search(query, top_k=5, category=None, jurisdiction=None, sources=None):\n",
    "                qv = st_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "                D, I = index.search(qv, top_k * 10)  # oversample, then post-filter\n",
    "                D, I = D[0], I[0]\n",
    "                res = []\n",
    "                for i, score in zip(I, D):\n",
    "                    row = meta.iloc[int(i)]\n",
    "                    if category and str(row[\"category\"]) != str(category):\n",
    "                        continue\n",
    "                    if jurisdiction and str(row[\"jurisdiction\"]) != str(jurisdiction):\n",
    "                        continue\n",
    "                    if sources and row[\"source\"] not in set(sources):\n",
    "                        continue\n",
    "                    res.append({\n",
    "                        \"rank\": len(res)+1,\n",
    "                        \"score\": float(score),\n",
    "                        \"doc_id\": row[\"doc_id\"],\n",
    "                        \"source\": row[\"source\"],\n",
    "                        \"category\": row[\"category\"],\n",
    "                        \"jurisdiction\": row[\"jurisdiction\"],\n",
    "                        \"text\": row[\"text\"]\n",
    "                    })\n",
    "                    if len(res) >= top_k:\n",
    "                        break\n",
    "                return res\n",
    "\n",
    "            _RET_CACHE.update({\"kind\": \"dense\", \"search\": _dense_search, \"meta\": meta})\n",
    "            return True\n",
    "        except Exception:\n",
    "            # fall through to TF-IDF fallback\n",
    "            pass\n",
    "\n",
    "    # Sparse (TF-IDF + cosine)\n",
    "    try:\n",
    "        import joblib\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        bundle = joblib.load(_ART_RAG_DIR / \"tfidf_index.pkl\")\n",
    "        vec        = bundle[\"vectorizer\"]\n",
    "        X          = bundle[\"matrix\"]\n",
    "        meta_path  = bundle[\"meta_path\"]  # <-- FIX: use meta_path, not \"meta\"\n",
    "        meta = pd.read_csv(meta_path)\n",
    "\n",
    "        def _sparse_search(query, top_k=5, category=None, jurisdiction=None, sources=None):\n",
    "            sims = cosine_similarity(vec.transform([query]), X).ravel()\n",
    "            order = sims.argsort()[::-1]\n",
    "            res = []\n",
    "            for idx in order:\n",
    "                row = meta.iloc[int(idx)]\n",
    "                if category and str(row[\"category\"]) != str(category):\n",
    "                    continue\n",
    "                if jurisdiction and str(row[\"jurisdiction\"]) != str(jurisdiction):\n",
    "                    continue\n",
    "                if sources and row[\"source\"] not in set(sources):\n",
    "                    continue\n",
    "                res.append({\n",
    "                    \"rank\": len(res)+1,\n",
    "                    \"score\": float(sims[idx]),\n",
    "                    \"doc_id\": row[\"doc_id\"],\n",
    "                    \"source\": row[\"source\"],\n",
    "                    \"category\": row[\"category\"],\n",
    "                    \"jurisdiction\": row[\"jurisdiction\"],\n",
    "                    \"text\": row[\"text\"]\n",
    "                })\n",
    "                if len(res) >= top_k:\n",
    "                    break\n",
    "            return res\n",
    "\n",
    "        _RET_CACHE.update({\"kind\": \"sparse\", \"search\": _sparse_search, \"meta\": meta})\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _ensure_generator() -> bool:\n",
    "    \"\"\"Load a small seq2seq model (use GEN from 4.2 if available; else FLAN-T5/T5).\"\"\"\n",
    "    if _GEN_CACHE[\"mdl\"] is not None:\n",
    "        return True\n",
    "    if \"GEN\" in globals() and isinstance(GEN, dict) and GEN.get(\"mdl\") is not None:\n",
    "        _GEN_CACHE.update(GEN)\n",
    "        return True\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        for name in [\"google/flan-t5-base\", \"google/flan-t5-small\", \"t5-small\"]:\n",
    "            try:\n",
    "                tok = AutoTokenizer.from_pretrained(name)\n",
    "                mdl = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
    "                _GEN_CACHE.update({\"tok\": tok, \"mdl\": mdl, \"name\": name})\n",
    "                return True\n",
    "            except Exception:\n",
    "                continue\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _build_prompt_from_passages(passages, category, city):\n",
    "    header = (\n",
    "        f\"You are EcoSort, a recycling assistant for {city}. \"\n",
    "        f\"Use ONLY the policy/disposal excerpts to produce clear recycling instructions.\\n\"\n",
    "        f\"Waste category: {category if category else 'Unknown'}.\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"• Prefer city-specific guidance.\\n\"\n",
    "        \"• If rules conflict, prefer the most explicit excerpt.\\n\"\n",
    "        \"• Keep output concise (5–8 bullet points).\\n\"\n",
    "        \"• End each bullet with a citation like [#N].\\n\"\n",
    "        \"• Do not invent facts.\\n\\n\"\n",
    "    )\n",
    "    ctx_lines = []\n",
    "    for i, p in enumerate(passages, start=1):\n",
    "        meta = f\"(source={p['source']}; doc_id={p['doc_id']}; cat={p['category']}; jur={p['jurisdiction']})\"\n",
    "        txt = _clean(p[\"text\"])\n",
    "        if len(txt) > 900:\n",
    "            txt = txt[:900] + \" ...\"\n",
    "        ctx_lines.append(f\"[#{i}] {meta}\\n{txt}\")\n",
    "    ctx = \"\\n\\n\".join(ctx_lines) if ctx_lines else \"No excerpts available.\"\n",
    "    task = (\n",
    "        \"\\n\\nWrite recycling instructions with sections:\\n\"\n",
    "        \"1) Steps\\n2) Do\\n3) Don't\\n4) Bin/Facility\\n5) Notes\\n\"\n",
    "        \"Each line must end with a valid citation [#N] when possible. \"\n",
    "        \"If no excerpts exist, say 'No city policy found' and provide general best-practice advice without citations.\"\n",
    "    )\n",
    "    return header + \"Policy excerpts:\\n\" + ctx + task\n",
    "\n",
    "def _generate_from_prompt(prompt, max_new_tokens=220, temperature=0.2) -> str:\n",
    "    if not _ensure_generator():\n",
    "        # Generator not available: graceful fallback\n",
    "        return \"No generator available. See policy excerpts above and follow their guidance.\"\n",
    "    from transformers import set_seed\n",
    "    set_seed(42)\n",
    "    tok, mdl = _GEN_CACHE[\"tok\"], _GEN_CACHE[\"mdl\"]\n",
    "    enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    out = mdl.generate(\n",
    "        **enc,\n",
    "        do_sample=(temperature > 0.0),\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=4 if temperature == 0.0 else 1,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tok.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def generate_recycling_instructions(waste_category: str):\n",
    "    \"\"\"\n",
    "    Generate detailed recycling instructions for a given waste category.\n",
    "\n",
    "    Args:\n",
    "        waste_category (str): Waste category (e.g., 'plastic', 'paper', ...)\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, list[dict]]: (instructions, policy_docs) where policy_docs is a compact list of retrieved passages.\n",
    "    \"\"\"\n",
    "    assert waste_category is not None and str(waste_category).strip() != \"\", \"Provide a non-empty waste category.\"\n",
    "    category = str(waste_category).strip()\n",
    "\n",
    "    # Preferred path: use the full RAG function from 4.2 if available\n",
    "    if \"rag_generate_instructions\" in globals():\n",
    "        out = rag_generate_instructions(\n",
    "            description=None,\n",
    "            image_label=category,\n",
    "            city=_CITY_DEFAULT,\n",
    "            top_k=_TOP_K_DEFAULT,\n",
    "            temperature=_TEMP_DEFAULT\n",
    "        )\n",
    "        policy_docs = [{\n",
    "            \"doc_id\": r[\"doc_id\"],\n",
    "            \"source\": r[\"source\"],\n",
    "            \"category\": r[\"category\"],\n",
    "            \"jurisdiction\": r[\"jurisdiction\"],\n",
    "            \"score\": r.get(\"score\", None),\n",
    "        } for r in out.get(\"retrieved\", [])]\n",
    "        return out.get(\"instructions\", \"\"), policy_docs\n",
    "\n",
    "    # Fallback path: use retriever + small generator directly\n",
    "    ok = _ensure_retriever()\n",
    "    assert ok, \"No retrieval index found. Run 4.1 (and optionally 4.2) first.\"\n",
    "\n",
    "    query = f\"Category: {category} | Jurisdiction: {_CITY_DEFAULT}\"\n",
    "    passages = _RET_CACHE[\"search\"](query, top_k=_TOP_K_DEFAULT, category=category, jurisdiction=_CITY_DEFAULT, sources=None)\n",
    "    if not passages:\n",
    "        # Widen filters if nothing found\n",
    "        passages = _RET_CACHE[\"search\"](query, top_k=_TOP_K_DEFAULT, category=category, jurisdiction=None, sources=None)\n",
    "\n",
    "    prompt = _build_prompt_from_passages(passages, category, _CITY_DEFAULT)\n",
    "    instructions = _generate_from_prompt(prompt, max_new_tokens=220, temperature=_TEMP_DEFAULT)\n",
    "\n",
    "    policy_docs = [{\n",
    "        \"doc_id\": p[\"doc_id\"],\n",
    "        \"source\": p[\"source\"],\n",
    "        \"category\": p[\"category\"],\n",
    "        \"jurisdiction\": p[\"jurisdiction\"],\n",
    "        \"score\": p.get(\"score\", None),\n",
    "    } for p in passages]\n",
    "\n",
    "    return instructions, policy_docs\n",
    "\n",
    "# ---- Usage example (uncomment to try) ----\n",
    "instr, docs = generate_recycling_instructions(\"plastic\")\n",
    "print(instr)\n",
    "pd.DataFrame(docs).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zkj_HTRRjNi"
   },
   "source": [
    "## Part 5: Integrated Waste Management Assistant\n",
    "\n",
    "In this section, you will integrate all three models into a unified waste management assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Znk62hFRjNi"
   },
   "source": [
    "### 5.1 Design Integration Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hj_U_JbZRjNi"
   },
   "outputs": [],
   "source": [
    "# ==== 5.1 — Integration Architecture (Image + Text + RAG) ====\n",
    "# Lightweight adapters for: ImageClassifier, TextClassifier, RAGGenerator\n",
    "# Orchestrator wires components and returns a consistent response schema.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception:\n",
    "    tf = None  # allow text-only path\n",
    "\n",
    "# ----------------------------- Data models (IO schema) -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class ClassificationResult:\n",
    "    source: str                      # \"image\" or \"text\"\n",
    "    category: Optional[str]\n",
    "    confidence: Optional[float]      # 0..1 when known; None if unknown\n",
    "    probs: Optional[Dict[str, float]] = None  # per-class (optional)\n",
    "    extra: Optional[Dict[str, Any]] = None    # any debug info\n",
    "\n",
    "@dataclass\n",
    "class InstructionResult:\n",
    "    instructions: str\n",
    "    retrieved: List[Dict[str, Any]]  # docs with ids/sources/score\n",
    "\n",
    "@dataclass\n",
    "class AssistantResponse:\n",
    "    decided_category: Optional[str]\n",
    "    decided_confidence: Optional[float]\n",
    "    chosen_source: str               # \"image\", \"text\", or \"fused\"\n",
    "    image_cls: Optional[ClassificationResult]\n",
    "    text_cls: Optional[ClassificationResult]\n",
    "    instructions: Optional[str]\n",
    "    citations: List[Dict[str, Any]]  # retrieved metadata\n",
    "    debug: Dict[str, Any]\n",
    "\n",
    "# ----------------------------- Image classifier adapter -----------------------------\n",
    "\n",
    "class ImageClassifierAdapter:\n",
    "    \"\"\"\n",
    "    Wraps a Keras CNN for single-image inference.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        keras_model,\n",
    "        class_names: List[str],\n",
    "        preprocess_input_fn=None,        # e.g., from tf.keras.applications.*.preprocess_input\n",
    "        img_size: Tuple[int, int]=(224, 224),\n",
    "    ):\n",
    "        assert keras_model is not None, \"Provide a trained Keras model.\"\n",
    "        self.model = keras_model\n",
    "        self.class_names = class_names or []\n",
    "        self.preprocess_input_fn = preprocess_input_fn or (lambda x: x)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def _load_image(self, image) -> \"np.ndarray\":\n",
    "        \"\"\"\n",
    "        Accepts a file path, bytes, PIL image, or a NumPy/TF tensor. Returns a float32 array ready for model input.\n",
    "        \"\"\"\n",
    "        if tf is None:\n",
    "            raise RuntimeError(\"TensorFlow not available for image preprocessing.\")\n",
    "        # If already a tensor/array of shape [H,W,3], trust the caller\n",
    "        if isinstance(image, np.ndarray):\n",
    "            arr = image\n",
    "        elif tf.is_tensor(image):\n",
    "            arr = image.numpy()\n",
    "        else:\n",
    "            # treat as file path-like / bytes\n",
    "            img_bytes = tf.io.read_file(str(image))\n",
    "            img = tf.io.decode_image(img_bytes, channels=3, expand_animations=False)\n",
    "            img = tf.image.resize(img, self.img_size)\n",
    "            arr = tf.cast(img, tf.float32).numpy()\n",
    "\n",
    "        # If the array seems to be in [0,1], scale to [0,255] for preprocess_input()\n",
    "        a_min, a_max = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
    "        if a_max <= 1.0 + 1e-6:\n",
    "            arr = arr * 255.0\n",
    "\n",
    "        # Apply model-specific preprocessing\n",
    "        arr = self.preprocess_input_fn(arr) if self.preprocess_input_fn else (arr / 255.0)\n",
    "\n",
    "        # Add batch dimension\n",
    "        if arr.ndim == 3:\n",
    "            arr = np.expand_dims(arr, 0)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "    def predict(self, image) -> ClassificationResult:\n",
    "        arr = self._load_image(image)\n",
    "        logits_or_probs = self.model.predict(arr, verbose=0)[0]\n",
    "\n",
    "        # If the model head isn't softmaxed, apply softmax to get probabilities\n",
    "        probs = logits_or_probs\n",
    "        if np.any(probs < 0) or not np.isclose(probs.sum(), 1.0, atol=1e-3):\n",
    "            e = np.exp(probs - np.max(probs))\n",
    "            probs = e / np.maximum(e.sum(), 1e-12)\n",
    "\n",
    "        idx = int(np.argmax(probs))\n",
    "        cat = self.class_names[idx] if self.class_names and 0 <= idx < len(self.class_names) else str(idx)\n",
    "        prob_map = {self.class_names[i] if i < len(self.class_names) else str(i): float(p)\n",
    "                    for i, p in enumerate(probs)}\n",
    "        extra_topk = sorted(prob_map.items(), key=lambda x: -x[1])[:3] if prob_map else None\n",
    "\n",
    "        return ClassificationResult(\n",
    "            source=\"image\",\n",
    "            category=cat,\n",
    "            confidence=float(probs[idx]),\n",
    "            probs=prob_map,\n",
    "            extra={\"topk\": extra_topk}\n",
    "        )\n",
    "\n",
    "# ----------------------------- Text classifier adapter -----------------------------\n",
    "\n",
    "class TextClassifierAdapter:\n",
    "    \"\"\"\n",
    "    Uses either:\n",
    "      - a direct function classify_waste_description(text) (label only),\n",
    "      - or a provided (model, vectorize_fn) pair supporting predict_proba/predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, classify_fn=None, model=None, vectorize_fn=None, label_list: Optional[List[str]]=None):\n",
    "        self.classify_fn = classify_fn  # returns a label string\n",
    "        self.model = model\n",
    "        self.vectorize_fn = vectorize_fn  # maps [str] -> feature matrix\n",
    "        self.labels = label_list\n",
    "\n",
    "    def predict(self, text: str) -> ClassificationResult:\n",
    "        # Preferred: model + vectorizer to get probabilities\n",
    "        if self.model is not None and self.vectorize_fn is not None:\n",
    "            X = self.vectorize_fn([text])\n",
    "            has_proba = hasattr(self.model, \"predict_proba\")\n",
    "            if has_proba:\n",
    "                proba = self.model.predict_proba(X)[0]\n",
    "                idx = int(np.argmax(proba))\n",
    "                cat = self.labels[idx] if self.labels else str(idx)\n",
    "                prob_map = {self.labels[i]: float(p) for i, p in enumerate(proba)} if self.labels else None\n",
    "                return ClassificationResult(\"text\", cat, float(proba[idx]), prob_map)\n",
    "            else:\n",
    "                y = self.model.predict(X)[0]\n",
    "                cat = self.labels[y] if (self.labels and isinstance(y, (int, np.integer))) else str(y)\n",
    "                return ClassificationResult(\"text\", cat, None, None)\n",
    "\n",
    "        # Fallback: label-only function\n",
    "        if self.classify_fn is not None:\n",
    "            try:\n",
    "                label = self.classify_fn(text)\n",
    "            except Exception:\n",
    "                label = None\n",
    "            return ClassificationResult(\"text\", label, None, None)\n",
    "\n",
    "        # Nothing available\n",
    "        return ClassificationResult(\"text\", None, None, None)\n",
    "\n",
    "# ----------------------------- RAG generator adapter -----------------------------\n",
    "\n",
    "class RAGGeneratorAdapter:\n",
    "    \"\"\"\n",
    "    Wraps your RAG pipeline (4.2). If rag_generate_instructions is not present,\n",
    "    optionally falls back to generate_recycling_instructions(category).\n",
    "    \"\"\"\n",
    "    def __init__(self, rag_fn=None, fallback_fn=None, city_default=\"Metro City\"):\n",
    "        self.rag_fn = rag_fn\n",
    "        self.fallback_fn = fallback_fn\n",
    "        self.city_default = city_default\n",
    "\n",
    "    def generate(self, description: Optional[str], category_hint: Optional[str], city: Optional[str]=None) -> InstructionResult:\n",
    "        city = city or self.city_default\n",
    "        if self.rag_fn is not None:\n",
    "            out = self.rag_fn(description=description, image_label=category_hint, city=city, top_k=5, temperature=0.0)\n",
    "            return InstructionResult(\n",
    "                instructions=out.get(\"instructions\", \"\"),\n",
    "                retrieved=out.get(\"retrieved\", []),\n",
    "            )\n",
    "        if self.fallback_fn is not None and category_hint:\n",
    "            instr, docs = self.fallback_fn(category_hint)\n",
    "            return InstructionResult(instructions=instr, retrieved=docs or [])\n",
    "        # No generator available\n",
    "        return InstructionResult(instructions=\"No instruction generator available.\", retrieved=[])\n",
    "\n",
    "# ----------------------------- Orchestrator -----------------------------\n",
    "\n",
    "class EcoSortAssistant:\n",
    "    \"\"\"\n",
    "    Routes inputs (image/description) to the right models, fuses predictions, and returns instructions + citations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_adapter: Optional[ImageClassifierAdapter]=None,\n",
    "        text_adapter: Optional[TextClassifierAdapter]=None,\n",
    "        rag_adapter: Optional[RAGGeneratorAdapter]=None,\n",
    "        agree_boost: float=0.1,        # confidence bonus when image & text agree\n",
    "        img_min_conf: float=0.40,      # minimum confidence to trust image-only prediction\n",
    "    ):\n",
    "        self.image_adapter = image_adapter\n",
    "        self.text_adapter = text_adapter\n",
    "        self.rag_adapter  = rag_adapter\n",
    "        self.agree_boost  = agree_boost\n",
    "        self.img_min_conf = img_min_conf\n",
    "\n",
    "    def _decide_category(self, img_res: Optional[ClassificationResult], txt_res: Optional[ClassificationResult]) -> Tuple[Optional[str], Optional[float], str]:\n",
    "        \"\"\"\n",
    "        Decision policy:\n",
    "          - If both predictions exist and agree, pick that label and boost confidence.\n",
    "          - Else pick the one with higher (known) confidence.\n",
    "          - If both confidences unknown, prefer text (more semantic), else image.\n",
    "        Returns: (category, confidence_estimate, chosen_source)\n",
    "        \"\"\"\n",
    "        # Neither available\n",
    "        if img_res is None and txt_res is None:\n",
    "            return None, None, \"none\"\n",
    "\n",
    "        img_conf = img_res.confidence if (img_res and img_res.confidence is not None) else 0.0\n",
    "        txt_conf = txt_res.confidence if (txt_res and txt_res.confidence is not None) else 0.0\n",
    "\n",
    "        # Agreement logic\n",
    "        if img_res and txt_res and img_res.category and txt_res.category and img_res.category == txt_res.category:\n",
    "            c = max(img_conf, txt_conf) + self.agree_boost\n",
    "            return img_res.category, float(min(c, 1.0)), \"fused\"\n",
    "\n",
    "        # Pick higher confidence among available\n",
    "        if (img_res and img_res.category) and (txt_res and txt_res.category):\n",
    "            if img_conf >= txt_conf:\n",
    "                return img_res.category, img_conf or None, \"image\"\n",
    "            else:\n",
    "                return txt_res.category, txt_conf or None, \"text\"\n",
    "\n",
    "        # Only one available\n",
    "        if img_res and img_res.category:\n",
    "            return img_res.category, img_conf or None, \"image\"\n",
    "        if txt_res and txt_res.category:\n",
    "            return txt_res.category, txt_conf or None, \"text\"\n",
    "\n",
    "        return None, None, \"none\"\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        image: Optional[Any]=None,         # path/bytes/array/tensor\n",
    "        description: Optional[str]=None,\n",
    "        city: str=\"Metro City\"\n",
    "    ) -> AssistantResponse:\n",
    "        img_res = None\n",
    "        txt_res = None\n",
    "        dbg: Dict[str, Any] = {\"errors\": []}\n",
    "\n",
    "        # 1) Image classification (optional)\n",
    "        if image is not None and self.image_adapter is not None:\n",
    "            try:\n",
    "                img_res = self.image_adapter.predict(image)\n",
    "            except Exception as e:\n",
    "                dbg[\"errors\"].append(f\"image_adapter: {e}\")\n",
    "                dbg[\"trace_image\"] = traceback.format_exc()\n",
    "\n",
    "        # 2) Text classification (optional)\n",
    "        if description and self.text_adapter is not None:\n",
    "            try:\n",
    "                txt_res = self.text_adapter.predict(description)\n",
    "            except Exception as e:\n",
    "                dbg[\"errors\"].append(f\"text_adapter: {e}\")\n",
    "                dbg[\"trace_text\"] = traceback.format_exc()\n",
    "\n",
    "        # 3) Decide category\n",
    "        decided_cat, decided_conf, chosen_source = self._decide_category(img_res, txt_res)\n",
    "\n",
    "        if decided_cat and chosen_source == \"image\" and (decided_conf is not None) and decided_conf < self.img_min_conf:\n",
    "            dbg[\"note\"] = \"Low image confidence; will still pass as hint to RAG but treat as uncertain.\"\n",
    "        \n",
    "        # 4) Generate instructions via RAG\n",
    "        instructions = None\n",
    "        citations: List[Dict[str, Any]] = []\n",
    "        if self.rag_adapter is not None:\n",
    "            try:\n",
    "                rag_out = self.rag_adapter.generate(description=description, category_hint=decided_cat, city=city)\n",
    "                instructions = rag_out.instructions\n",
    "                citations = rag_out.retrieved\n",
    "            except Exception as e:\n",
    "                dbg[\"errors\"].append(f\"rag_adapter: {e}\")\n",
    "                dbg[\"trace_rag\"] = traceback.format_exc()\n",
    "\n",
    "        return AssistantResponse(\n",
    "            decided_category=decided_cat,\n",
    "            decided_confidence=decided_conf,\n",
    "            chosen_source=chosen_source,\n",
    "            image_cls=img_res,\n",
    "            text_cls=txt_res,\n",
    "            instructions=instructions,\n",
    "            citations=citations,\n",
    "            debug=dbg\n",
    "        )\n",
    "\n",
    "# ----------------------------- Wiring it up (plug your components) -----------------------------\n",
    "\n",
    "# 1) Image adapter (if you trained a CNN in 2.x)\n",
    "try:\n",
    "    _class_names = None\n",
    "    if \"PIPELINE_INFO\" in globals() and isinstance(PIPELINE_INFO, dict) and \"class_names\" in PIPELINE_INFO:\n",
    "        _class_names = PIPELINE_INFO[\"class_names\"]\n",
    "    elif \"class_names\" in globals():\n",
    "        _class_names = class_names\n",
    "\n",
    "    _img_size = (224, 224)\n",
    "    if tf is not None and \"train_ds\" in globals():\n",
    "        # infer from dataset spec if available\n",
    "        spec = train_ds.element_spec[0].shape\n",
    "        if spec[1] is not None and spec[2] is not None:\n",
    "            _img_size = (int(spec[1]), int(spec[2]))\n",
    "\n",
    "    IMAGE_ADAPTER = ImageClassifierAdapter(\n",
    "        keras_model=model if \"model\" in globals() else None,\n",
    "        class_names=_class_names or [],\n",
    "        preprocess_input_fn=globals().get(\"preprocess_input\", None),\n",
    "        img_size=_img_size\n",
    "    ) if \"model\" in globals() else None\n",
    "except Exception:\n",
    "    IMAGE_ADAPTER = None\n",
    "\n",
    "# 2) Text adapter (uses classify_waste_description() from 3.4 by default)\n",
    "TEXT_ADAPTER = TextClassifierAdapter(\n",
    "    classify_fn=globals().get(\"classify_waste_description\", None),\n",
    "    model=None,               # optionally provide a trained classic model here\n",
    "    vectorize_fn=None,        # and its vectorizer to get probabilities\n",
    "    label_list=None\n",
    ")\n",
    "\n",
    "# 3) RAG adapter (prefer 4.2 rag_generate_instructions; fallback to 4.4 generate_recycling_instructions)\n",
    "RAG_ADAPTER = RAGGeneratorAdapter(\n",
    "    rag_fn=globals().get(\"rag_generate_instructions\", None),\n",
    "    fallback_fn=globals().get(\"generate_recycling_instructions\", None),\n",
    "    city_default=\"Metro City\"\n",
    ")\n",
    "\n",
    "# 4) Orchestrator instance\n",
    "ecosort = EcoSortAssistant(\n",
    "    image_adapter=IMAGE_ADAPTER,\n",
    "    text_adapter=TEXT_ADAPTER,\n",
    "    rag_adapter=RAG_ADAPTER,\n",
    "    agree_boost=0.10,\n",
    "    img_min_conf=0.40\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVKgKn85RjNi"
   },
   "source": [
    "### 5.2 Implement Integrated Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJYkJ8LIRjNi"
   },
   "outputs": [],
   "source": [
    "# ==== 5.2 — Integrated Assistant: single entrypoint function ====\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "import numpy as np\n",
    "\n",
    "def _to_numpy_rgb_any(x) -> np.ndarray:\n",
    "    \"\"\"Accepts a path/bytes/PIL.Image/NumPy/TF tensor and returns RGB uint8 numpy array [H,W,3].\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        _has_tf = True\n",
    "    except Exception:\n",
    "        _has_tf = False\n",
    "\n",
    "    def _to_uint8_rgb(arr: np.ndarray) -> np.ndarray:\n",
    "        # Ensure float arrays are scaled properly\n",
    "        if arr.ndim == 2:                      # (H,W) -> (H,W,1)\n",
    "            arr = arr[..., None]\n",
    "        if arr.shape[-1] == 1:                 # grayscale -> RGB\n",
    "            arr = np.repeat(arr, 3, axis=-1)\n",
    "        if arr.dtype != np.uint8:\n",
    "            a_min, a_max = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
    "            if a_max <= 1.0 + 1e-6:            # [0,1] -> [0,255]\n",
    "                arr = arr * 255.0\n",
    "            arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "        return arr\n",
    "\n",
    "    # Already a numpy array\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return _to_uint8_rgb(x)\n",
    "\n",
    "    # TF tensor\n",
    "    if _has_tf and hasattr(tf, \"is_tensor\") and tf.is_tensor(x):\n",
    "        return _to_uint8_rgb(x.numpy())\n",
    "\n",
    "    # Bytes / file-like / path → PIL\n",
    "    from PIL import Image\n",
    "    if isinstance(x, (str, Path)):\n",
    "        img = Image.open(str(x)).convert(\"RGB\")\n",
    "    elif isinstance(x, (bytes, bytearray)):\n",
    "        import io\n",
    "        img = Image.open(io.BytesIO(x)).convert(\"RGB\")\n",
    "    elif hasattr(x, \"read\"):  # file-like\n",
    "        img = Image.open(x).convert(\"RGB\")\n",
    "    else:\n",
    "        # Last resort: try PIL on whatever this is\n",
    "        img = Image.open(x).convert(\"RGB\")\n",
    "    return np.array(img, dtype=np.uint8)\n",
    "\n",
    "def waste_management_assistant(input_data, input_type: str = \"image\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Integrated waste management assistant that processes either images or text descriptions\n",
    "    and returns waste classification and recycling instructions.\n",
    "\n",
    "    Args:\n",
    "        input_data: Either an image file path/array or a text description\n",
    "        input_type (str): \"image\" or \"text\"\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"category\": str|None,\n",
    "            \"confidence\": float|None,\n",
    "            \"source\": \"image\"|\"text\"|\"fused\",\n",
    "            \"instructions\": str|None,\n",
    "            \"citations\": list,         # retrieved doc metadata if RAG used\n",
    "            \"debug\": dict              # any fallback notes/errors\n",
    "        }\n",
    "    \"\"\"\n",
    "    input_type = (input_type or \"image\").strip().lower()\n",
    "    debug = {}\n",
    "\n",
    "    # --- Preferred path: use the orchestrator from 5.1 if available ---\n",
    "    if \"ecosort\" in globals():\n",
    "        try:\n",
    "            if input_type == \"image\":\n",
    "                img_arr = _to_numpy_rgb_any(input_data)\n",
    "                out = ecosort.process(image=img_arr, description=None, city=\"Metro City\")\n",
    "            elif input_type == \"text\":\n",
    "                desc = str(input_data) if input_data is not None else \"\"\n",
    "                out = ecosort.process(image=None, description=desc, city=\"Metro City\")\n",
    "            else:\n",
    "                raise ValueError(\"input_type must be 'image' or 'text'.\")\n",
    "\n",
    "            return {\n",
    "                \"category\": out.decided_category,\n",
    "                \"confidence\": out.decided_confidence,\n",
    "                \"source\": out.chosen_source,\n",
    "                \"instructions\": out.instructions,\n",
    "                \"citations\": out.citations,\n",
    "                \"debug\": out.debug,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            debug[\"orchestrator_error\"] = str(e)\n",
    "\n",
    "    # --- Fallback path: do it manually with available components ---\n",
    "    category = None\n",
    "    confidence: Optional[float] = None\n",
    "    source = input_type\n",
    "    instructions = None\n",
    "    citations = []\n",
    "\n",
    "    # 1) Classification\n",
    "    if input_type == \"image\":\n",
    "        # Try CNN (model + class_names)\n",
    "        if \"model\" in globals():\n",
    "            try:\n",
    "                arr = _to_numpy_rgb_any(input_data)\n",
    "                # Determine input size from model if possible\n",
    "                try:\n",
    "                    H, W = int(model.input_shape[1]), int(model.input_shape[2])\n",
    "                except Exception:\n",
    "                    H, W = 224, 224\n",
    "                from PIL import Image\n",
    "                arr_resized = np.array(Image.fromarray(arr).resize((W, H)))\n",
    "                # Preprocess\n",
    "                preprocess = globals().get(\"preprocess_input\", None)\n",
    "                x = arr_resized.astype(np.float32)\n",
    "                if preprocess is not None:\n",
    "                    # Many preprocess_input expect 0..255\n",
    "                    x = preprocess(x)\n",
    "                else:\n",
    "                    x = x / 255.0\n",
    "                x = np.expand_dims(x, 0)  # [1,H,W,3]\n",
    "                probs = model.predict(x, verbose=0)[0]\n",
    "                # If not a softmaxed head, softmax here\n",
    "                if np.any(probs < 0) or not np.isclose(probs.sum(), 1.0, atol=1e-3):\n",
    "                    e = np.exp(probs - np.max(probs))\n",
    "                    probs = e / np.maximum(e.sum(), 1e-12)\n",
    "                idx = int(np.argmax(probs))\n",
    "                class_list = None\n",
    "                if \"PIPELINE_INFO\" in globals() and isinstance(PIPELINE_INFO, dict):\n",
    "                    class_list = PIPELINE_INFO.get(\"class_names\", None)\n",
    "                if class_list is None and \"class_names\" in globals():\n",
    "                    class_list = globals()[\"class_names\"]\n",
    "                category = class_list[idx] if class_list and 0 <= idx < len(class_list) else str(idx)\n",
    "                confidence = float(probs[idx])\n",
    "            except Exception as e:\n",
    "                debug[\"image_cls_error\"] = str(e)\n",
    "        else:\n",
    "            debug[\"image_cls_note\"] = \"CNN model not found (run 2.x).\"\n",
    "\n",
    "    elif input_type == \"text\":\n",
    "        # Preferred: use classify_waste_description from 3.4\n",
    "        if \"classify_waste_description\" in globals():\n",
    "            try:\n",
    "                category = classify_waste_description(str(input_data))\n",
    "            except Exception as e:\n",
    "                debug[\"text_cls_error\"] = str(e)\n",
    "        else:\n",
    "            debug[\"text_cls_note\"] = \"Text classifier not found (run 3.x).\"\n",
    "    else:\n",
    "        raise ValueError(\"input_type must be 'image' or 'text'.\")\n",
    "\n",
    "    # 2) Instruction generation (RAG preferred)\n",
    "    try:\n",
    "        if \"rag_generate_instructions\" in globals():\n",
    "            if input_type == \"text\":\n",
    "                out = rag_generate_instructions(description=str(input_data), image_label=category, city=\"Metro City\", top_k=5, temperature=0.0)\n",
    "            else:\n",
    "                out = rag_generate_instructions(description=None, image_label=category, city=\"Metro City\", top_k=5, temperature=0.0)\n",
    "            instructions = out.get(\"instructions\", None)\n",
    "            citations = out.get(\"retrieved\", [])\n",
    "        elif \"generate_recycling_instructions\" in globals() and category:\n",
    "            instructions, citations = generate_recycling_instructions(category)\n",
    "        else:\n",
    "            instructions = \"No instruction generator available. Please run Section 4.x to build the RAG index.\"\n",
    "            citations = []\n",
    "    except Exception as e:\n",
    "        debug[\"rag_error\"] = str(e)\n",
    "\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"confidence\": confidence,\n",
    "        \"source\": source,\n",
    "        \"instructions\": instructions,\n",
    "        \"citations\": citations,\n",
    "        \"debug\": debug,\n",
    "    }\n",
    "\n",
    "# --- Examples (safe to run) ---\n",
    "print(waste_management_assistant(\"Clear PET plastic bottle with cap\", input_type=\"text\"))\n",
    "\n",
    "try:\n",
    "    # Replace with a real file if you want to test images\n",
    "    print(waste_management_assistant(\"/path/to/sample.jpg\", input_type=\"image\"))\n",
    "except Exception as _e:\n",
    "    print({\"note\": \"Image demo skipped (file not found). Provide a real image path to test.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbav93z8RjNi"
   },
   "source": [
    "### 5.3 Evaluate the Integrated System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj1IlIljRjNi"
   },
   "outputs": [],
   "source": [
    "# ==== 5.3 — Evaluate the Integrated System (image + text + RAG quality) ====\n",
    "# What this cell does:\n",
    "# - IMAGE: uses test image file paths if available (from Section 1.3 stratified split).\n",
    "#   Falls back to direct model.eval on test_ds if orchestrated assistant can't be exercised on raw files.\n",
    "# - TEXT : recreates the 80/20 stratified split used in 3.1 and evaluates the assistant on the test texts.\n",
    "# - RAG  : small qualitative probe on a few items to check citation rate and output length.\n",
    "\n",
    "from pathlib import Path\n",
    "import json, re, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --------------------------- Helpers ---------------------------\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    s = re.sub(r\"\\s+\",\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _pick(cols, candidates):\n",
    "    norm = {c: c.strip().lower().replace(\" \", \"_\") for c in cols}\n",
    "    for want in candidates:\n",
    "        for orig, n in norm.items():\n",
    "            if n == want or want in n: return orig\n",
    "    return None\n",
    "\n",
    "def _citation_rate(text: str, k: int) -> float:\n",
    "    \"\"\"Share of bullet/numbered lines ending with a valid [#N], 1<=N<=k.\"\"\"\n",
    "    if not isinstance(text, str) or k <= 0: return 0.0\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    total = 0; good = 0\n",
    "    for ln in lines:\n",
    "        if re.match(r\"^(\\d+\\.|[-•])\\s+\", ln):\n",
    "            total += 1\n",
    "            m = re.search(r\"\\[#(\\d+)\\]\\s*$\", ln)\n",
    "            if m:\n",
    "                n = int(m.group(1))\n",
    "                if 1 <= n <= k: good += 1\n",
    "    return (good/total) if total else 0.0\n",
    "\n",
    "def _plot_confusion(cm, labels, title):\n",
    "    if cm is None or labels is None: \n",
    "        return\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(labels))\n",
    "    plt.xticks(ticks, range(len(labels)))\n",
    "    plt.yticks(ticks, range(len(labels)))\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def _fmt_metric(x):\n",
    "    return \"n/a\" if x is None or (isinstance(x, float) and np.isnan(x)) else f\"{x:.4f}\"\n",
    "\n",
    "# --------------------------- IMAGE EVALUATION ---------------------------\n",
    "\n",
    "IMAGE_RESULTS = {}\n",
    "if \"waste_management_assistant\" in globals():\n",
    "    if \"test_paths\" in globals() and \"test_labels\" in globals():\n",
    "        # Evaluate the integrated assistant end-to-end on raw files (preferred)\n",
    "        class_names = None\n",
    "        if \"PIPELINE_INFO\" in globals() and isinstance(PIPELINE_INFO, dict):\n",
    "            class_names = PIPELINE_INFO.get(\"class_names\", None)\n",
    "        if class_names is None and \"class_names\" in globals():\n",
    "            class_names = globals()[\"class_names\"]\n",
    "        assert class_names is not None, \"class_names not found; ensure 1.3 defined them.\"\n",
    "\n",
    "        paths = list(test_paths)\n",
    "        y_true_idx = np.array(test_labels, dtype=int)\n",
    "        y_true = [class_names[i] for i in y_true_idx]\n",
    "\n",
    "        # Limit for speed if desired\n",
    "        MAX_IMAGES = None  # e.g., 200 to cap the run\n",
    "        if MAX_IMAGES is not None:\n",
    "            paths = paths[:MAX_IMAGES]\n",
    "            y_true = y_true[:MAX_IMAGES]\n",
    "\n",
    "        preds, confs = [], []\n",
    "        t0 = time.time()\n",
    "        for p in paths:\n",
    "            out = waste_management_assistant(p, input_type=\"image\")\n",
    "            preds.append(out.get(\"category\"))\n",
    "            confs.append(out.get(\"confidence\"))\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Filter to aligned labels only (ignore None or unknown)\n",
    "        keep = [i for i, pr in enumerate(preds) if pr in class_names]\n",
    "        y_true_f = [y_true[i] for i in keep]\n",
    "        y_pred_f = [preds[i]   for i in keep]\n",
    "\n",
    "        img_acc = accuracy_score(y_true_f, y_pred_f) if y_true_f else np.nan\n",
    "        img_f1  = f1_score(y_true_f, y_pred_f, average=\"macro\") if y_true_f else np.nan\n",
    "        cm      = confusion_matrix(y_true_f, y_pred_f, labels=class_names) if y_true_f else None\n",
    "\n",
    "        IMAGE_RESULTS = {\n",
    "            \"n_evaluated\": len(y_true_f),\n",
    "            \"accuracy\": float(img_acc) if not np.isnan(img_acc) else None,\n",
    "            \"macro_f1\": float(img_f1) if not np.isnan(img_f1) else None,\n",
    "            \"avg_confidence\": float(np.nanmean([c for c in confs if c is not None])) if confs else None,\n",
    "            \"elapsed_sec\": round(t1 - t0, 2),\n",
    "            \"labels\": class_names,\n",
    "            \"confusion\": cm,\n",
    "        }\n",
    "        print(f\"[IMAGE] Assistant accuracy={_fmt_metric(IMAGE_RESULTS['accuracy'])} | macroF1={_fmt_metric(IMAGE_RESULTS['macro_f1'])} | n={IMAGE_RESULTS['n_evaluated']}\")\n",
    "        _plot_confusion(cm, class_names, \"Image — Assistant Confusion Matrix\")\n",
    "\n",
    "    elif \"test_ds\" in globals() and \"model\" in globals():\n",
    "        # Fallback: evaluate the CNN model directly on the preprocessed test_ds\n",
    "        print(\"[IMAGE] test_paths not available; evaluating the CNN directly on test_ds.\")\n",
    "        eval_metrics = model.evaluate(test_ds, verbose=1)\n",
    "        print(\"Model test metrics:\", dict(zip(model.metrics_names, eval_metrics)))\n",
    "    else:\n",
    "        print(\"[IMAGE] Skipped: no test set found.\")\n",
    "\n",
    "else:\n",
    "    print(\"[IMAGE] Skipped: `waste_management_assistant` not defined. Run 5.2 first.\")\n",
    "\n",
    "# --------------------------- TEXT EVALUATION ---------------------------\n",
    "\n",
    "TEXT_RESULTS = {}\n",
    "_df_text_eval = None  # keep for RAG probe if available\n",
    "if \"waste_management_assistant\" in globals():\n",
    "    # Locate dataset\n",
    "    CAND = [\n",
    "        Path(\"/mnt/data/waste_descriptions.csv\"),\n",
    "        Path(\"data/waste_descriptions.csv\"),\n",
    "        Path(\"waste_descriptions.csv\"),\n",
    "        Path.home() / \"Downloads\" / \"waste_descriptions.csv\",\n",
    "    ]\n",
    "    desc_path = next((p for p in CAND if p.exists()), None)\n",
    "    assert desc_path is not None, \"waste_descriptions.csv not found.\"\n",
    "\n",
    "    df = pd.read_csv(desc_path)\n",
    "    _df_text_eval = df.copy()  # save for RAG probe\n",
    "    text_col = _pick(df.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "    label_col= _pick(df.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "    assert text_col and label_col, \"Need text and label columns in waste_descriptions.csv\"\n",
    "\n",
    "    df[text_col]  = df[text_col].astype(str).map(_clean_text)\n",
    "    df[label_col] = df[label_col].astype(str).str.strip()\n",
    "    df = df[df[text_col].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "    # Reuse saved label order if available (from 3.1)\n",
    "    labels_path = Path(\"artifacts/text_features/label_mapping.json\")\n",
    "    if labels_path.exists():\n",
    "        labels = json.loads(labels_path.read_text())[\"labels\"]\n",
    "        le = LabelEncoder().fit(labels)\n",
    "    else:\n",
    "        le = LabelEncoder().fit(df[label_col].values)\n",
    "        labels = le.classes_.tolist()\n",
    "\n",
    "    y_ids = le.transform(df[label_col].values)\n",
    "    # Recreate the 80/20 split from 3.1 (seed=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[text_col].values, y_ids, test_size=0.20, random_state=42, stratify=y_ids\n",
    "    )\n",
    "    y_true = [labels[i] for i in y_test]\n",
    "\n",
    "    # Limit to keep RAG calls reasonable (assistant generates instructions)\n",
    "    MAX_TEXT = None  # e.g., 400 to cap the run; None = evaluate all\n",
    "    if MAX_TEXT is not None:\n",
    "        X_test = X_test[:MAX_TEXT]\n",
    "        y_true = y_true[:MAX_TEXT]\n",
    "\n",
    "    preds = []\n",
    "    t0 = time.time()\n",
    "    for x in X_test:\n",
    "        out = waste_management_assistant(str(x), input_type=\"text\")\n",
    "        preds.append(out.get(\"category\"))\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Filter to known labels\n",
    "    keep = [i for i, pr in enumerate(preds) if pr in labels]\n",
    "    y_true_f = [y_true[i] for i in keep]\n",
    "    y_pred_f = [preds[i]   for i in keep]\n",
    "\n",
    "    txt_acc = accuracy_score(y_true_f, y_pred_f) if y_true_f else np.nan\n",
    "    txt_f1  = f1_score(y_true_f, y_pred_f, average=\"macro\") if y_true_f else np.nan\n",
    "    cm      = confusion_matrix(y_true_f, y_pred_f, labels=labels) if y_true_f else None\n",
    "\n",
    "    TEXT_RESULTS = {\n",
    "        \"n_evaluated\": len(y_true_f),\n",
    "        \"accuracy\": float(txt_acc) if not np.isnan(txt_acc) else None,\n",
    "        \"macro_f1\": float(txt_f1) if not np.isnan(txt_f1) else None,\n",
    "        \"elapsed_sec\": round(t1 - t0, 2),\n",
    "        \"labels\": labels,\n",
    "        \"confusion\": cm,\n",
    "    }\n",
    "    print(f\"[TEXT] Assistant accuracy={_fmt_metric(TEXT_RESULTS['accuracy'])} | macroF1={_fmt_metric(TEXT_RESULTS['macro_f1'])} | n={TEXT_RESULTS['n_evaluated']}\")\n",
    "    _plot_confusion(cm, labels, \"Text — Assistant Confusion Matrix\")\n",
    "\n",
    "else:\n",
    "    print(\"[TEXT] Skipped: `waste_management_assistant` not defined. Run 5.2 first.\")\n",
    "\n",
    "# --------------------------- RAG QUALITY (SMALL PROBE) ---------------------------\n",
    "\n",
    "RAG_RESULTS = {}\n",
    "def _try_rag(description, category_hint):\n",
    "    \"\"\"Call the assistant and return (instructions, retrieved_n, cite_rate, length_words).\"\"\"\n",
    "    out = waste_management_assistant(description, input_type=\"text\")\n",
    "    instr = out.get(\"instructions\") or \"\"\n",
    "    retrieved = out.get(\"citations\") or []\n",
    "    return instr, len(retrieved), _citation_rate(instr, len(retrieved)), len(instr.split())\n",
    "\n",
    "try:\n",
    "    # Load df independently to avoid dependency on TEXT block\n",
    "    if _df_text_eval is None:\n",
    "        CAND = [\n",
    "            Path(\"/mnt/data/waste_descriptions.csv\"),\n",
    "            Path(\"data/waste_descriptions.csv\"),\n",
    "            Path(\"waste_descriptions.csv\"),\n",
    "            Path.home() / \"Downloads\" / \"waste_descriptions.csv\",\n",
    "        ]\n",
    "        desc_path = next((p for p in CAND if p.exists()), None)\n",
    "        assert desc_path is not None, \"waste_descriptions.csv not found.\"\n",
    "        _df_text_eval = pd.read_csv(desc_path)\n",
    "\n",
    "    # Column detection for RAG probe\n",
    "    text_col_probe = _pick(_df_text_eval.columns, [\"description\",\"text\",\"item_description\",\"waste_description\"])\n",
    "    label_col_probe= _pick(_df_text_eval.columns, [\"category\",\"label\",\"waste_category\",\"class\"])\n",
    "    assert text_col_probe, \"Text column not found for RAG probe.\"\n",
    "    if label_col_probe is None:\n",
    "        _df_text_eval[\"_tmp_label\"] = \"unknown\"\n",
    "        label_col_probe = \"_tmp_label\"\n",
    "\n",
    "    _df_text_eval[text_col_probe] = _df_text_eval[text_col_probe].astype(str).map(_clean_text)\n",
    "    per_cat = 2\n",
    "    sample_rows = []\n",
    "    for cat, group in _df_text_eval.groupby(label_col_probe):\n",
    "        sub = group.sample(min(per_cat, len(group)), random_state=7)\n",
    "        for _, r in sub.iterrows():\n",
    "            sample_rows.append({\"category\": str(cat), \"description\": str(r[text_col_probe])})\n",
    "    sample_rows = sample_rows[:12]\n",
    "\n",
    "    recs = []\n",
    "    for row in sample_rows:\n",
    "        instr, k, cite_rate, n_words = _try_rag(row[\"description\"], row[\"category\"])\n",
    "        recs.append({\n",
    "            \"category\": row[\"category\"],\n",
    "            \"cite_rate\": cite_rate,\n",
    "            \"retrieved_k\": k,\n",
    "            \"len_words\": n_words,\n",
    "            \"preview\": instr[:220] + (\"...\" if len(instr) > 220 else \"\")\n",
    "        })\n",
    "    rag_df = pd.DataFrame(recs)\n",
    "    RAG_RESULTS = {\n",
    "        \"avg_citation_rate\": float(rag_df[\"cite_rate\"].mean()) if len(rag_df) else None,\n",
    "        \"avg_len_words\": float(rag_df[\"len_words\"].mean()) if len(rag_df) else None,\n",
    "        \"n_samples\": len(rag_df)\n",
    "    }\n",
    "    print(f\"[RAG] avg citation rate={_fmt_metric(RAG_RESULTS['avg_citation_rate'])} | avg length={rag_df['len_words'].mean():.1f} | n={RAG_RESULTS['n_samples']}\")\n",
    "    display(rag_df.head(10))\n",
    "except Exception as e:\n",
    "    print(\"[RAG] Probe skipped:\", e)\n",
    "\n",
    "# --------------------------- Summary ---------------------------\n",
    "\n",
    "SUMMARY = {\n",
    "    \"image\": {k: (v if not isinstance(v, np.ndarray) else \"confusion_matrix_shown_above\") for k,v in IMAGE_RESULTS.items()},\n",
    "    \"text\":  {k: (v if not isinstance(v, np.ndarray) else \"confusion_matrix_shown_above\") for k,v in TEXT_RESULTS.items()},\n",
    "    \"rag\":   RAG_RESULTS,\n",
    "}\n",
    "print(\"\\n=== Integrated System Evaluation Summary ===\")\n",
    "print(json.dumps(SUMMARY, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (loslab)",
   "language": "python",
   "name": "loslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
